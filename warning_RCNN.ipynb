{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#008000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>GOOD NEWS:</strong> If we change the number of channels from 166x166x30 to 1 it runs (yep, I know it looks like a big diff but I think it should be 1 (not too sure)) \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#008000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>THEREFORE IGNORE ALL THE WARNINGS BELOW:</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ff0000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>Warning:</strong> Do not run in vs code! My computer just rebooted \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFA500; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>However:</strong> You can run everything until model = RCNN() in 2. Define the Neural Network \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFA500; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    Also, the second time I ran it, my laptop just crashed for a few minutes, but didn't reboot, so maybe if you have a better laptop it works in yours \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes of things to add:\n",
    "\n",
    "- We are minimizing loss, however we want to maximize F1 score. Do that in the cross validation. How? No f clue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import pandas as pd\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab, mount Google Drive, for local environments, get local path (github)\n",
    "\n",
    "# Change with the appropriate path. Log in into Drive and create the folders with the data\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder0_path = '/content/drive/My Drive/solar_jets/data0'\n",
    "    folder1_path = '/content/drive/My Drive/solar_jets/data1'\n",
    "else:\n",
    "    # For local environments like VS Code\n",
    "    folder0_path = './data0'\n",
    "    folder1_path = './data1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Very naive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # in self.files, only add f if it has 30 samples\n",
    "        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n",
    "\n",
    "    # Check if the file has 30 samples, if not, don't include it in self.files (see above)\n",
    "    def _check_file_shape(self, file):\n",
    "        file_path = os.path.join(self.data_dir, file)\n",
    "        data = np.load(file_path)['arr_0']\n",
    "        return data.shape == (166, 166, 30)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        data = np.load(file_path)['arr_0']\n",
    "\n",
    "        # Rearrange dimensions to (30, 166, 166) for PyTorch (insted of (166, 166, 30))\n",
    "        data = np.moveaxis(data, -1, 0)\n",
    "        \n",
    "        # assign label 1 if data is from data1, 0 if from data0\n",
    "        label = 1.0 if 'data1' in self.data_dir else 0.0\n",
    "        return torch.from_numpy(data).float(), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run to get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = NPZDataset(folder1_path)\n",
    "train_data0 = NPZDataset(folder0_path)\n",
    "train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll see how train_data can be treated just like lists\n",
    "\n",
    "- One element in the list for each sequence of images\n",
    "- Each element of the list has a tuple of two elements, the first the array (166,30,30) and the second the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples y=1:  117\n",
      "samples y=0:  10\n",
      "samples total  127\n"
     ]
    }
   ],
   "source": [
    "print(\"samples y=1: \",len(train_data1))\n",
    "print(\"samples y=0: \",len(train_data0))\n",
    "print(\"samples total \",len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Shape torch.Size([30, 166, 166]), Label: 1.0\n",
      "Sample 0: Shape torch.Size([30, 166, 166]), Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample {0}: Shape {train_data1[0][0].shape}, Label: {train_data1[0][1]}\")\n",
    "print(f\"Sample {0}: Shape {train_data0[0][0].shape}, Label: {train_data0[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that we have 4 different minibatches, each with 32 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches:  4\n",
      "Batch 0: samples 32\n",
      "Batch 1: samples 32\n",
      "Batch 2: samples 32\n",
      "Batch 3: samples 31\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches: \",len(train_loader))\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}: samples {x.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader is designed to iterate over batches of data rather than individual samples, so when we try to access for example the first mini-batch, we get an error\n",
    "\n",
    "However, note how before we could iterate over it\n",
    "\n",
    "<span style=\"color:green\">Yes, the cell below gives an error</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "\n",
    "        # 1. Input layers\n",
    "        #self.input_layer=torch.nn.Linear(in_features=166*166*30, out_features=166*166*30)      # not sure if this is needed\n",
    "        #self.norm=torch.nn.BatchNorm2d(166*166*30)        #not sure this is sufficent for batch renormalization\n",
    "\n",
    "        # 2. Convolutional layers\n",
    "\n",
    "        # Three conv layers\n",
    "        # according to gpt, in_channels=1 because in the sequence, it is still gray scale, not rgb\n",
    "        self.conv1 = self.create_conv_layer(in_channels=30, out_channels=64)\n",
    "        self.conv2 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv3 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "\n",
    "        # First maxpool\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Three conv layers\n",
    "        self.conv4 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv5 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv6 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "\n",
    "        # Second maxpool\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Flatten\n",
    "        self.flatten=torch.nn.Flatten()\n",
    "\n",
    "        # Each time we apply pooling, height and width are divided by 2\n",
    "        self.flat_size = 64 * int(np.floor(166/2/2)) * int(np.floor(166/2/2))\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        self.lin=torch.nn.Linear(self.flat_size, 1024)\n",
    "        self.drop=torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        #LSTM\n",
    "        # we set batch_first=True to have batch_size as first dimension: (30,166,166)\n",
    "        self.lstm = torch.nn.LSTM(input_size=1024, hidden_size=512, num_layers=1, batch_first=True)\n",
    "\n",
    "        #output: for binary classification softmax is not needed\n",
    "        #self.output_layer=torch.nn.Softmax(dim=1)\n",
    "        self.output = torch.nn.Linear(512, 1)\n",
    "\n",
    "    # To make the code more clear, let's use this function to create conv layers\n",
    "    def create_conv_layer(self, in_channels, out_channels):\n",
    "        conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        relu = torch.nn.ReLU()\n",
    "        norm = torch.nn.BatchNorm2d(out_channels)\n",
    "        return torch.nn.Sequential(conv, relu, norm)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Define the forward pass\n",
    "\n",
    "        # 1. Input layers\n",
    "        #x=self.input_layer(x)\n",
    "        #x=self.norm(x)\n",
    "\n",
    "        # 2. Convolutional layers\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "\n",
    "        # First maxpool\n",
    "        x=self.pool1(x)\n",
    "\n",
    "        # Three conv layers\n",
    "        x=self.conv4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.conv6(x)\n",
    "\n",
    "        # Second maxpool\n",
    "        x=self.pool2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x=self.flatten(x)\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        x=self.lin(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        #LSTM\n",
    "        #x = x.view(x.size(0), -1, self.flat_size)\n",
    "        #x = x.view(x.size(0), 30, -1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        #lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        #output\n",
    "        out=self.output(lstm_out)\n",
    "        \n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ff0000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>Warning:</strong> The code will crash here &darr; &darr; &darr; &darr; \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ff0000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>Warning:</strong> The code will crash here &uarr; &uarr; &uarr; &uarr; \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Parameters\n",
    "\n",
    "There is a lot that can be done here to get very cool hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrien\\anaconda3\\envs\\envADA\\lib\\site-packages\\torch\\cuda\\__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "criterion = torch.nn.functional.cross_entropy\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train epoch script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    lr_history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        threshold = 0.5\n",
    "        pred = (output >= threshold).float()\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss_float = loss.item()\n",
    "        accuracy_float = correct / len(data)\n",
    "\n",
    "        loss_history.append(loss_float)\n",
    "        accuracy_history.append(accuracy_float)\n",
    "        lr_history.append(scheduler.get_last_lr()[0])\n",
    "        \n",
    "        # this is the if statement of the lab, but it doesn't work for me. it makes no sense to me tbh\n",
    "        #if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
    "        # this doesn't work bad, but it prints twice per epoch\n",
    "        if batch_idx == (len(train_loader.dataset) // len(data) - 1):\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "                f\"batch_loss={loss_float:0.2e} \"\n",
    "                f\"batch_acc={accuracy_float:0.3f} \"\n",
    "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \"\n",
    "            )\n",
    "\n",
    "    return loss_history, accuracy_history, lr_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the Network ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-002 batch_loss=1.05e+02 batch_acc=0.875 lr=9.397e-04 \n",
      "Train Epoch: 1-003 batch_loss=9.67e+01 batch_acc=0.871 lr=8.946e-04 \n",
      "Train Epoch: 2-002 batch_loss=1.05e+02 batch_acc=0.781 lr=7.008e-04 \n",
      "Train Epoch: 2-003 batch_loss=9.63e+01 batch_acc=0.871 lr=6.227e-04 \n",
      "Train Epoch: 3-002 batch_loss=1.08e+02 batch_acc=0.906 lr=3.773e-04 \n",
      "Train Epoch: 3-003 batch_loss=9.35e+01 batch_acc=0.710 lr=2.992e-04 \n",
      "Train Epoch: 4-002 batch_loss=1.12e+02 batch_acc=1.000 lr=1.054e-04 \n",
      "Train Epoch: 4-003 batch_loss=9.25e+01 batch_acc=0.903 lr=6.026e-05 \n",
      "Train Epoch: 5-002 batch_loss=1.00e+02 batch_acc=0.969 lr=0.000e+00 \n",
      "Train Epoch: 5-003 batch_loss=1.03e+02 batch_acc=0.968 lr=6.819e-06 \n"
     ]
    }
   ],
   "source": [
    "lr_history = []\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device)\n",
    "    train_loss_history.extend(train_loss)\n",
    "    train_acc_history.extend(train_acc)\n",
    "    lr_history.extend(lrs)\n",
    "\n",
    "    # val_loss, val_acc = validate(model, device, val_loader, criterion)\n",
    "    # val_loss_history.append(val_loss)\n",
    "    # val_acc_history.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the Model\n",
    "\n",
    "Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solarjets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
