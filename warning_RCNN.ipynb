{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ff0000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>Warning:</strong> Do not run in vs code! My computer just rebooted \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFA500; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>However:</strong> You can run everything until model = RCNN() in 2. Define the Neural Network \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFA500; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    Also, the second time I ran it, my laptop just crashed for a few minutes, but didn't reboot, so maybe if you have a better laptop it works in yours \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes of things to add:\n",
    "\n",
    "- We are minimizing loss, however we want to maximize F1 score. Do that in the cross validation. How? No f clue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import pandas as pd\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with the appropriate path\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder1_path = '/content/drive/My Drive/folder1'\n",
    "    folder2_path = '/content/drive/My Drive/folder2'\n",
    "else:\n",
    "    # For local environments like VS Code\n",
    "    folder0_path = './data0'\n",
    "    folder1_path = './data1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Very naive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # in self.files, only add f if it has 30 samples\n",
    "        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n",
    "\n",
    "    # Check if the file has 30 samples, if not, don't include it in self.files (see above)\n",
    "    def _check_file_shape(self, file):\n",
    "        file_path = os.path.join(self.data_dir, file)\n",
    "        data = np.load(file_path)['arr_0']\n",
    "        return data.shape == (166, 166, 30)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        data = np.load(file_path)['arr_0']\n",
    "        \n",
    "        # assign label 1 if data is from data1, 0 if from data0\n",
    "        label = 1 if 'data1' in self.data_dir else 0\n",
    "        return torch.from_numpy(data).float(), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run to get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = NPZDataset(folder1_path)\n",
    "train_data0 = NPZDataset(folder0_path)\n",
    "train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll see how train_data can be treated just like lists\n",
    "\n",
    "- One element in the list for each sequence of images\n",
    "- Each element of the list has a tuple of two elements, the first the array (166,30,30) and the second the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples y=1:  117\n",
      "samples y=0:  10\n",
      "samples total  127\n"
     ]
    }
   ],
   "source": [
    "print(\"samples y=1: \",len(train_data1))\n",
    "print(\"samples y=0: \",len(train_data0))\n",
    "print(\"samples total \",len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Shape torch.Size([166, 166, 30]), Label: 1\n",
      "Sample 0: Shape torch.Size([166, 166, 30]), Label: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample {0}: Shape {train_data1[0][0].shape}, Label: {train_data1[0][1]}\")\n",
    "print(f\"Sample {0}: Shape {train_data0[0][0].shape}, Label: {train_data0[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that we have 4 different minibatches, each with 32 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches:  4\n",
      "Batch 0: samples 32\n",
      "Batch 1: samples 32\n",
      "Batch 2: samples 32\n",
      "Batch 3: samples 31\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches: \",len(train_loader))\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}: samples {x.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader is designed to iterate over batches of data rather than individual samples, so when we try to access for example the first mini-batch, we get an error\n",
    "\n",
    "However, note how before we could iterate over it\n",
    "\n",
    "<span style=\"color:green\">Yes, the cell below gives an error</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "\n",
    "        # 1. Input layers\n",
    "        self.input_layer=torch.nn.Linear(in_features=166*166*30, out_features=166*166*30)      # not sure if this is needed\n",
    "        self.norm=torch.nn.BatchNorm2d()        #not sure this is sufficent for batch renormalization\n",
    "\n",
    "        # 2. Convolutional layers\n",
    "\n",
    "        # Three conv layers\n",
    "        self.conv1 = self.create_conv_layer(in_channels=30, out_channels=64)\n",
    "        self.conv2 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv3 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "\n",
    "        # First maxpool\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Three conv layers\n",
    "        self.conv4 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv5 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv6 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "\n",
    "        # Second maxpool\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Flatten\n",
    "        self.flat1=torch.nn.Flatten()\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        self.lin=torch.nn.Linear(in_features=2304, out_features=1024)\n",
    "        self.drop=torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        #LSTM\n",
    "        self.lstm=torch.nn.LSTM(input_size=1024,output_size=512, num_layers=1)\n",
    "\n",
    "        #output\n",
    "        self.output_layer=torch.nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    # To make the code more clear, let's use this function to create conv layers\n",
    "    def create_conv_layer(self, in_channels, out_channels):\n",
    "        conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        relu = torch.nn.ReLU()\n",
    "        norm = torch.nn.BatchNorm2d(out_channels)\n",
    "        return torch.nn.Sequential(conv, relu, norm)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Define the forward pass\n",
    "\n",
    "        # 1. Input layers\n",
    "        x=self.input_layer(x)\n",
    "        x=self.norm(x)\n",
    "\n",
    "        # 2. Convolutional layers\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "\n",
    "        # First maxpool\n",
    "        x=self.pool1(x)\n",
    "\n",
    "        # Three conv layers\n",
    "        x=self.conv4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.conv6(x)\n",
    "\n",
    "        # Second maxpool\n",
    "        x=self.pool2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x=self.flat1(x)\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        x=self.lin(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        #LSTM\n",
    "        x=self.lstm(x)\n",
    "\n",
    "        #output\n",
    "        x=self.output_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ff0000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>Warning:</strong> The code will crash here &darr; &darr; &darr; &darr; \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ff0000; color:#ffffff; padding:10px; border-radius:5px;\">\n",
    "    <strong>Warning:</strong> The code will crash here &uarr; &uarr; &uarr; &uarr; \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Parameters\n",
    "\n",
    "There is a lot that can be done here to get very cool hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "criterion = torch.nn.functional.cross_entropy\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the Network ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_history = []\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc, lrs = train_epoch(\n",
    "        model, optimizer, scheduler, criterion, train_loader, epoch, device\n",
    "    )\n",
    "    train_loss_history.extend(train_loss)\n",
    "    train_acc_history.extend(train_acc)\n",
    "    lr_history.extend(lrs)\n",
    "\n",
    "    val_loss, val_acc = validate(model, device, val_loader, criterion)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the Model\n",
    "\n",
    "Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solarjets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
