{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 1350px; height: 30px; background-color: green;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "##### **Cross Validation**\n",
    "- Currently, we are using cross-entropy for the criterion, but according to ChatGPT, it has nothing to do with cross-validation, and I think that makes sense.\n",
    "- We want to use cross-validation to somehow max the F1 score\n",
    "  - We are minimizing loss, but somehow we want to maximize F1 score during cross validation (see feedback Project 1)\n",
    "  - Currently the `train_acc_history` doesn't work well. At each epoch it returns the acc for ALL minibatches. Therefore, at the end of the 5 epochs instead of containing 5 values, it contains 20 (5 epochs x 4 minibatches). I think that with cross-validation we have to take the average of them ?? \n",
    "\n",
    "##### **Other**\n",
    "- Parameter tuning: get cool hyperparameters (criterion, number of epochs, optimizer, scheduler)\n",
    "- Data Augmentation: transformations (rotations, translations, cutouts, zoom in, zoom out...)\n",
    "- Multimodal data: currently we only train the image itself. Maybe it's useful to train other parameters such as the date or the location, as Sophie said that if a jet occurs in a location, it is more likely that another jet occurs in that same location.\n",
    "- Batch size set to 32. Change it?\n",
    "\n",
    "##### **Model Testing**\n",
    "- We have to divide data into training, validation, and test. BUT, as we are using cross validation we don't need the validation set I think???\n",
    "- Need to evaluate and test model\n",
    "\n",
    "##### **Network Model**\n",
    "- I didn't include the first two layers from the paper (input layer and BRN). Should we? CAREFUL AS THIS MAY REBOOT THE LAPTOP\n",
    "- Batch **re**normalization ?\n",
    "- Part of me thinks that this model is better when we have small images but with a lot of samples, because using 127 datapoints (which is low but not that low), it only takes 2 min to run even when running on CPU, which seems like so little time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 1350px; height: 30px; background-color: green;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries used all the time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Operating system libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab, mount Google Drive, for local environments, get local path (github)\n",
    "\n",
    "# Change with the appropriate path. Log in into Drive and create the folders with the data\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder0_path = '/content/drive/My Drive/solar_jets/data0'\n",
    "    folder1_path = '/content/drive/My Drive/solar_jets/data1'\n",
    "else:\n",
    "    # For local environments like VS Code\n",
    "    folder0_path = './data0'\n",
    "    folder1_path = './data1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Very naive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # in self.files, only add f if it has 30 samples\n",
    "        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n",
    "\n",
    "    # Check if the file has 30 samples, if not, don't include it in self.files (see above)\n",
    "    def _check_file_shape(self, file):\n",
    "        file_path = os.path.join(self.data_dir, file)\n",
    "        data = np.load(file_path)['arr_0']\n",
    "        return data.shape == (166, 166, 30)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        data = np.load(file_path)['arr_0']\n",
    "\n",
    "        # Rearrange dimensions to (30, 166, 166) for PyTorch (insted of (166, 166, 30))\n",
    "        data = np.moveaxis(data, -1, 0)\n",
    "\n",
    "        # assign label 1 if data is from data1, 0 if from data0\n",
    "        label = 1.0 if 'data1' in self.data_dir else 0.0\n",
    "        return torch.from_numpy(data).float(), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run to get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = NPZDataset(folder1_path)\n",
    "train_data0 = NPZDataset(folder0_path)\n",
    "train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll see how train_data can be treated just like lists\n",
    "\n",
    "- One element in the list for each sequence of images\n",
    "- Each element of the list has a tuple of two elements, the first the array (166,30,30) and the second the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples y=1:  866\n",
      "samples y=0:  846\n",
      "samples total  1712\n"
     ]
    }
   ],
   "source": [
    "print(\"samples y=1: \",len(train_data1))\n",
    "print(\"samples y=0: \",len(train_data0))\n",
    "print(\"samples total \",len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Shape torch.Size([30, 166, 166]), Label: 1.0\n",
      "Sample 0: Shape torch.Size([30, 166, 166]), Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample {0}: Shape {train_data1[0][0].shape}, Label: {train_data1[0][1]}\")\n",
    "print(f\"Sample {0}: Shape {train_data0[0][0].shape}, Label: {train_data0[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_data))  # 80% of data for training\n",
    "test_size = len(train_data) - train_size  # remaining 20% for testing\n",
    "train_dataset, test_dataset = random_split(train_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that we have 4 different minibatches, each with 32 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches:  43\n",
      "Batch 0: samples 32\n",
      "Batch 1: samples 32\n",
      "Batch 2: samples 32\n",
      "Batch 3: samples 32\n",
      "Batch 4: samples 32\n",
      "Batch 5: samples 32\n",
      "Batch 6: samples 32\n",
      "Batch 7: samples 32\n",
      "Batch 8: samples 32\n",
      "Batch 9: samples 32\n",
      "Batch 10: samples 32\n",
      "Batch 11: samples 32\n",
      "Batch 12: samples 32\n",
      "Batch 13: samples 32\n",
      "Batch 14: samples 32\n",
      "Batch 15: samples 32\n",
      "Batch 16: samples 32\n",
      "Batch 17: samples 32\n",
      "Batch 18: samples 32\n",
      "Batch 19: samples 32\n",
      "Batch 20: samples 32\n",
      "Batch 21: samples 32\n",
      "Batch 22: samples 32\n",
      "Batch 23: samples 32\n",
      "Batch 24: samples 32\n",
      "Batch 25: samples 32\n",
      "Batch 26: samples 32\n",
      "Batch 27: samples 32\n",
      "Batch 28: samples 32\n",
      "Batch 29: samples 32\n",
      "Batch 30: samples 32\n",
      "Batch 31: samples 32\n",
      "Batch 32: samples 32\n",
      "Batch 33: samples 32\n",
      "Batch 34: samples 32\n",
      "Batch 35: samples 32\n",
      "Batch 36: samples 32\n",
      "Batch 37: samples 32\n",
      "Batch 38: samples 32\n",
      "Batch 39: samples 32\n",
      "Batch 40: samples 32\n",
      "Batch 41: samples 32\n",
      "Batch 42: samples 25\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches: \",len(train_loader))\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}: samples {x.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader is designed to iterate over batches of data rather than individual samples, so when we try to access for example the first mini-batch as `train_loader[0]`, we get an error.\n",
    "\n",
    "However, note how before we could iterate over it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "\n",
    "        # 1. Input layers\n",
    "        #self.input_layer=torch.nn.Linear(in_features=166*166*30, out_features=166*166*30)      # not sure if this is needed\n",
    "        #self.norm=torch.nn.BatchNorm2d(166*166*30)        #not sure this is sufficent for batch renormalization\n",
    "\n",
    "        # 2. Convolutional layers\n",
    "\n",
    "        # Three conv layers\n",
    "        # according to gpt, in_channels=1 because in the sequence, it is still gray scale, not rgb\n",
    "        self.conv1 = self.create_conv_layer(in_channels=30, out_channels=64)\n",
    "        self.conv2 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv3 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "\n",
    "        # First maxpool\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Three conv layers\n",
    "        self.conv4 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv5 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "        self.conv6 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
    "\n",
    "        # Second maxpool\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Flatten\n",
    "        self.flatten=torch.nn.Flatten()\n",
    "\n",
    "        # Each time we apply pooling, height and width are divided by 2\n",
    "        self.flat_size = 64 * int(np.floor(166/2/2)) * int(np.floor(166/2/2))\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        self.lin=torch.nn.Linear(self.flat_size, 1024)\n",
    "        self.drop=torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        #LSTM\n",
    "        # we set batch_first=True to have batch_size as first dimension: (30,166,166)\n",
    "        self.lstm = torch.nn.LSTM(input_size=1024, hidden_size=512, num_layers=1, batch_first=True)\n",
    "\n",
    "        #output: for binary classification softmax is not needed\n",
    "        #self.output_layer=torch.nn.Softmax(dim=1)\n",
    "        self.output = torch.nn.Linear(512, 1)\n",
    "\n",
    "    # To make the code more clear, let's use this function to create conv layers\n",
    "    def create_conv_layer(self, in_channels, out_channels):\n",
    "        conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        relu = torch.nn.ReLU()\n",
    "        norm = torch.nn.BatchNorm2d(out_channels)\n",
    "        return torch.nn.Sequential(conv, relu, norm)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Define the forward pass\n",
    "\n",
    "        # 1. Input layers\n",
    "        #x=self.input_layer(x)\n",
    "        #x=self.norm(x)\n",
    "\n",
    "        # 2. Convolutional layers\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "\n",
    "        # First maxpool\n",
    "        x=self.pool1(x)\n",
    "\n",
    "        # Three conv layers\n",
    "        x=self.conv4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.conv6(x)\n",
    "\n",
    "        # Second maxpool\n",
    "        x=self.pool2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x=self.flatten(x)\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        x=self.lin(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        #LSTM\n",
    "        #x = x.view(x.size(0), -1, self.flat_size)\n",
    "        #x = x.view(x.size(0), 30, -1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        #lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        #output\n",
    "        out=self.output(lstm_out)\n",
    "        \n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Parameters\n",
    "\n",
    "There is a lot that can be done here to get very cool hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Epoch Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    lr_history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        threshold = 0.5\n",
    "        pred = (output >= threshold).float()\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss_float = loss.item()\n",
    "        accuracy_float = correct / len(data)\n",
    "\n",
    "        loss_history.append(loss_float)\n",
    "        accuracy_history.append(accuracy_float)\n",
    "        lr_history.append(scheduler.get_last_lr()[0])\n",
    "        \n",
    "        # this is the if statement of the lab, but it doesn't work for me. it makes no sense to me tbh\n",
    "        #if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
    "        # this doesn't work bad, but it just prints the last batch of the epoch\n",
    "        if batch_idx == len(train_loader) - 1:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "                f\"batch_loss={loss_float:0.2e} \"\n",
    "                f\"batch_acc={accuracy_float:0.3f} \"\n",
    "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \"\n",
    "            )\n",
    "\n",
    "    return loss_history, accuracy_history, lr_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = (output >= 0.5).float()  # threshold for binary classification\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * test_accuracy:.2f}%)\\n')\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the Network 😀😀😀😀😀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History\n",
    "lr_history = []\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "\n",
    "# Validation History\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device)\n",
    "    train_loss_history.extend(train_loss)\n",
    "    train_acc_history.extend(train_acc)\n",
    "    lr_history.extend(lrs)\n",
    "\n",
    "    # Test the model after each epoch\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion, device)\n",
    "    val_loss_history.append(test_loss)\n",
    "    val_acc_history.append(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (20,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/carloscc/Library/CloudStorage/OneDrive-epfl.ch/Fall 2023/CS-433 Machine learning/GitHub/Projects/Project 2/warning_RCNN.ipynb Cell 31\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carloscc/Library/CloudStorage/OneDrive-epfl.ch/Fall%202023/CS-433%20Machine%20learning/GitHub/Projects/Project%202/warning_RCNN.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carloscc/Library/CloudStorage/OneDrive-epfl.ch/Fall%202023/CS-433%20Machine%20learning/GitHub/Projects/Project%202/warning_RCNN.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Plotting learning rate history\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/carloscc/Library/CloudStorage/OneDrive-epfl.ch/Fall%202023/CS-433%20Machine%20learning/GitHub/Projects/Project%202/warning_RCNN.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m axs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mplot(\u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, num_epochs \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), lr_history, marker\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mo\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carloscc/Library/CloudStorage/OneDrive-epfl.ch/Fall%202023/CS-433%20Machine%20learning/GitHub/Projects/Project%202/warning_RCNN.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mLearning Rate History\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carloscc/Library/CloudStorage/OneDrive-epfl.ch/Fall%202023/CS-433%20Machine%20learning/GitHub/Projects/Project%202/warning_RCNN.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/solarjets/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/solarjets/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/anaconda3/envs/solarjets/lib/python3.11/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (20,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plotting learning rate history\n",
    "axs[0].plot(range(1, num_epochs + 1), lr_history, marker='o')\n",
    "axs[0].set_title('Learning Rate History')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Learning Rate')\n",
    "\n",
    "# Plotting training loss history\n",
    "axs[1].plot(range(1, num_epochs + 1), train_loss_history, marker='o', color='r')\n",
    "axs[1].set_title('Training Loss History')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Loss')\n",
    "\n",
    "# Plotting training accuracy history\n",
    "axs[2].plot(range(1, num_epochs + 1), train_acc_history, marker='o', color='g')\n",
    "axs[2].set_title('Training Accuracy History')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solarjets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
