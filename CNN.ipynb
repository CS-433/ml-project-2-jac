{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnQt-_vumnL-"
      },
      "source": [
        "# CNN model (from \"Fast Solar Image Classification Using Deep Learning and its Importance for Automation in Solar Physics\" - Convolutional neural network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCDgIqkym_T_"
      },
      "source": [
        "## 0. Load modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsMvUQwfAzrX",
        "outputId": "0caf720d-9ce7-4bd5-96d2-0f7c1d401211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/179.2 kB\u001b[0m \u001b[31m749.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/179.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n",
            "Collecting sunpy\n",
            "  Downloading sunpy-5.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astropy!=5.1.0,>=5.0.6 in /usr/local/lib/python3.10/dist-packages (from sunpy) (5.3.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from sunpy) (1.23.5)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from sunpy) (23.2)\n",
            "Collecting parfive[ftp]>=2.0.0 (from sunpy)\n",
            "  Downloading parfive-2.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy!=5.1.0,>=5.0.6->sunpy) (2.0.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy!=5.1.0,>=5.0.6->sunpy) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy) (3.9.1)\n",
            "Collecting aioftp>=0.17.1 (from parfive[ftp]>=2.0.0->sunpy)\n",
            "  Downloading aioftp-0.21.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->parfive[ftp]>=2.0.0->sunpy) (3.6)\n",
            "Installing collected packages: aioftp, parfive, sunpy\n",
            "Successfully installed aioftp-0.21.4 parfive-2.0.2 sunpy-5.1.0\n"
          ]
        }
      ],
      "source": [
        "# Main libraries used all the time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Operating system libraries\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Machine learning libraries\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torchvision.transforms import v2 # library to define the transforms\n",
        "\n",
        "!pip install torcheval  # needs to run on google colab, on local machine you can just \"pip install torcheval\" in the terminal\n",
        "from torcheval.metrics.functional import multiclass_f1_score\n",
        "\n",
        "# Visualization and debugging\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "!pip install sunpy\n",
        "from sunpy.visualization.colormaps import color_tables as ct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnVx_RzCmoLy"
      },
      "source": [
        "## 1. Import the data, create the dataset, define the Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSzwdjC9nLS5"
      },
      "source": [
        "### 1.1 Declare data path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkofDkmFm4ru",
        "outputId": "b932a993-e98a-47dd-ae19-5a342e611799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# For Google Colab, mount Google Drive, for local environments, get local path (github)\n",
        "\n",
        "# Change with the appropriate path. Log in into Drive and create the folders with the data\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Carlos\n",
        "    #folder0_path = '/content/drive/My Drive/solar_jets/data0'\n",
        "    #folder0_test_path = '/content/drive/My Drive/solar_jets/data0_test'\n",
        "    #folder1_path = '/content/drive/My Drive/solar_jets/data1'\n",
        "    #folder1_test_path = '/content/drive/My Drive/solar_jets/data1_test'\n",
        "\n",
        "    # Julie\n",
        "    folder0_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0'\n",
        "    folder1_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1'\n",
        "    folder0_test_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0_test'\n",
        "    folder1_test_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1_test'\n",
        "else:\n",
        "    # For local environments like VS Code\n",
        "    folder0_path = './data0'\n",
        "    folder1_path = './data1'\n",
        "    folder0_test_path = './data0_test'\n",
        "    folder1_test_path = './data1_test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBdJqW0EnPE1"
      },
      "source": [
        "### 1.2 Declare the class and the transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j86rBUk7nHk4"
      },
      "outputs": [],
      "source": [
        "class NPZDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # in self.files, only add f if it has 30 samples\n",
        "        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n",
        "        self.transform = transform\n",
        "\n",
        "    # Check if the file has 30 samples, if not, don't include it in self.files (see above)\n",
        "    def _check_file_shape(self, file):\n",
        "        file_path = os.path.join(self.data_dir, file)\n",
        "        data = np.load(file_path)['arr_0']\n",
        "        return data.shape == (166, 166, 30)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
        "        data = np.load(file_path)['arr_0']\n",
        "\n",
        "        # Rearrange dimensions to (30, 166, 166) for PyTorch (insted of (166, 166, 30))\n",
        "        data = np.moveaxis(data, -1, 0)\n",
        "\n",
        "        # assign label 1 if data is from data1, 0 if from data0\n",
        "        label = 1.0 if 'data1' in self.data_dir else 0.0\n",
        "        return torch.from_numpy(data).float(), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rpgEJukSnT8B"
      },
      "outputs": [],
      "source": [
        "mean = 51.6644\n",
        "std = 62.7087\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True), #equivalent to transforms.ToTensor()\n",
        "    v2.Normalize((mean,), (std,)),\n",
        "    #v2.RandomResizedCrop(size=(140, 140)),\n",
        "    #v2.RandomAffine(degrees=(-90,90), translate=(0.3, 0.3), scale=(0.8, 1.2))\n",
        "    # add other transforms if needed\n",
        "])\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True), #equivalent to transforms.ToTensor()\n",
        "    v2.Normalize((mean,), (std,)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNjMEErinbHL"
      },
      "source": [
        "### 1.3 Get the data, declare the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rVT2xKkkngaO"
      },
      "outputs": [],
      "source": [
        "train_data1 = NPZDataset(folder1_path, transform=train_transform)\n",
        "train_data0 = NPZDataset(folder0_path, transform=train_transform)\n",
        "train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])\n",
        "\n",
        "test_data1 = NPZDataset(folder1_test_path, transform=test_transform)\n",
        "test_data0 = NPZDataset(folder0_test_path, transform=test_transform)\n",
        "test_data = torch.utils.data.ConcatDataset([test_data1, test_data0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-5Lz4kpOng8k"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy7kgCSBnk8z"
      },
      "source": [
        "### 1.4 Check that we have our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHEvX339np-L",
        "outputId": "d2100f4e-0131-45bd-93ce-27e357e3dd6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training samples:  1384\n",
            "Total testing samples:  345\n",
            "Batch 0: samples 16\n",
            "Batch 1: samples 16\n",
            "Batch 2: samples 16\n",
            "Batch 3: samples 16\n",
            "Batch 4: samples 16\n",
            "Batch 5: samples 16\n",
            "Batch 6: samples 16\n",
            "Batch 7: samples 16\n",
            "Batch 8: samples 16\n",
            "Batch 9: samples 16\n",
            "Batch 10: samples 16\n",
            "Batch 11: samples 16\n",
            "Batch 12: samples 16\n",
            "Batch 13: samples 16\n",
            "Batch 14: samples 16\n",
            "Batch 15: samples 16\n",
            "Batch 16: samples 16\n",
            "Batch 17: samples 16\n",
            "Batch 18: samples 16\n",
            "Batch 19: samples 16\n",
            "Batch 20: samples 16\n",
            "Batch 21: samples 16\n",
            "Batch 22: samples 16\n",
            "Batch 23: samples 16\n",
            "Batch 24: samples 16\n",
            "Batch 25: samples 16\n",
            "Batch 26: samples 16\n",
            "Batch 27: samples 16\n",
            "Batch 28: samples 16\n",
            "Batch 29: samples 16\n",
            "Batch 30: samples 16\n",
            "Batch 31: samples 16\n",
            "Batch 32: samples 16\n",
            "Batch 33: samples 16\n",
            "Batch 34: samples 16\n",
            "Batch 35: samples 16\n",
            "Batch 36: samples 16\n",
            "Batch 37: samples 16\n",
            "Batch 38: samples 16\n",
            "Batch 39: samples 16\n",
            "Batch 40: samples 16\n",
            "Batch 41: samples 16\n",
            "Batch 42: samples 16\n",
            "Batch 43: samples 16\n",
            "Batch 44: samples 16\n",
            "Batch 45: samples 16\n",
            "Batch 46: samples 16\n",
            "Batch 47: samples 16\n",
            "Batch 48: samples 16\n",
            "Batch 49: samples 16\n",
            "Batch 50: samples 16\n",
            "Batch 51: samples 16\n",
            "Batch 52: samples 16\n",
            "Batch 53: samples 16\n",
            "Batch 54: samples 16\n",
            "Batch 55: samples 16\n",
            "Batch 56: samples 16\n",
            "Batch 57: samples 16\n",
            "Batch 58: samples 16\n",
            "Batch 59: samples 16\n",
            "Batch 60: samples 16\n",
            "Batch 61: samples 16\n",
            "Batch 62: samples 16\n",
            "Batch 63: samples 16\n",
            "Batch 64: samples 16\n",
            "Batch 65: samples 16\n",
            "Batch 66: samples 16\n",
            "Batch 67: samples 16\n",
            "Batch 68: samples 16\n",
            "Batch 69: samples 16\n",
            "Batch 70: samples 16\n",
            "Batch 71: samples 16\n",
            "Batch 72: samples 16\n",
            "Batch 73: samples 16\n",
            "Batch 74: samples 16\n",
            "Batch 75: samples 16\n",
            "Batch 76: samples 16\n",
            "Batch 77: samples 16\n",
            "Batch 78: samples 16\n",
            "Batch 79: samples 16\n",
            "Batch 80: samples 16\n",
            "Batch 81: samples 16\n",
            "Batch 82: samples 16\n",
            "Batch 83: samples 16\n",
            "Batch 84: samples 16\n",
            "Batch 85: samples 16\n",
            "Batch 86: samples 8\n"
          ]
        }
      ],
      "source": [
        "print(\"Total training samples: \",len(train_data))\n",
        "print(\"Total testing samples: \", len(test_data))\n",
        "\n",
        "for i, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: samples {x.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPzYSdV8nyP1"
      },
      "source": [
        "## 2. Define the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nD6ZJI1_BIhG"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        ## Convolution layers\n",
        "\n",
        "        # 2 conv 64 + max pool\n",
        "        self.conv1 = self.create_conv_layer(in_channels=30, out_channels=64)\n",
        "        self.conv2 = self.create_conv_layer(in_channels=64, out_channels=64)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        # 2 conv 128 + max pool\n",
        "        self.conv3 = self.create_conv_layer(in_channels=64, out_channels=128)\n",
        "        self.conv4 = self.create_conv_layer(in_channels=128, out_channels=128)\n",
        "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        # 2 conv 256 + max pool\n",
        "        self.conv5 = self.create_conv_layer(in_channels=128, out_channels=256)\n",
        "        self.conv6 = self.create_conv_layer(in_channels=256, out_channels=256)\n",
        "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        # 2 conv 512 + max pool\n",
        "        self.conv7 = self.create_conv_layer(in_channels=256, out_channels=512)\n",
        "        self.conv8 = self.create_conv_layer(in_channels=512, out_channels=512)\n",
        "        self.pool4 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        # 2 conv 512 + max pool\n",
        "        self.conv9 = self.create_conv_layer(in_channels=512, out_channels=512)\n",
        "        self.conv10 = self.create_conv_layer(in_channels=512, out_channels=512)\n",
        "        self.pool5 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        ## Fully connected layers\n",
        "\n",
        "        # linear + ReLU + dropot (p=0.5)\n",
        "        self.linear1 = torch.nn.Linear(in_features=512*5*5, out_features=1600)\n",
        "        self.R1 = torch.nn.ReLU()\n",
        "        self.DO1 = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "        # linear + ReLU + dropot (p=0.5)\n",
        "        self.linear2 = torch.nn.Linear(in_features=1600, out_features=1600)\n",
        "        self.R2 = torch.nn.ReLU()\n",
        "        self.DO2 = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "        # Final linear layer\n",
        "        self.linear3 = torch.nn.Linear(in_features=1600, out_features=2)\n",
        "        self.sm = torch.nn.Softmax()\n",
        "\n",
        "\n",
        "        # To make the code more clear, let's use this function to create conv layers\n",
        "    def create_conv_layer(self, in_channels, out_channels):\n",
        "        conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        norm = torch.nn.BatchNorm2d(out_channels)\n",
        "        relu = torch.nn.ReLU()\n",
        "        return torch.nn.Sequential(conv, norm, relu)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Define the forward pass\n",
        "        ## Convolution layers\n",
        "        x=self.conv1(x)\n",
        "        x=self.conv2(x)\n",
        "        x=self.pool1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        x = self.conv10(x)\n",
        "        x = self.pool5(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        ## Fully connected layers\n",
        "        x = self.linear1(x)\n",
        "        x = self.R1(x)\n",
        "        x = self.DO1(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "        x = self.R2(x)\n",
        "        x = self.DO2(x)\n",
        "\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U-G7B7UMRPLc"
      },
      "outputs": [],
      "source": [
        "model = CNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On00kDTMn57D"
      },
      "source": [
        "## 3. Define the training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AujB7DZDAzrj"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
        "    model.train()\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "    lr_history = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.float().to(device), target.float().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        argmax_output = torch.argmax(output, dim=1).float().clone().detach().requires_grad_(True) # weird but equivalent to torch.tensor(...)\n",
        "\n",
        "        # Debugging\n",
        "        if batch_idx == 24:\n",
        "          print(\"output: \", output)\n",
        "          print(\"argmax output: \", argmax_output)\n",
        "          print(\"target: \", target)\n",
        "        loss = criterion(argmax_output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = torch.argmax(output, dim=1).float().clone().detach().requires_grad_(False)\n",
        "\n",
        "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss_float = loss.item()\n",
        "        accuracy_float = correct / len(data)\n",
        "\n",
        "        loss_history.append(loss_float)\n",
        "        accuracy_history.append(accuracy_float)\n",
        "        lr_history.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "        # this is the if statement of the lab, but it doesn't work for me. it makes no sense to me tbh\n",
        "        #if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
        "        # this doesn't work bad, but it just prints the last batch of the epoch\n",
        "        \"\"\"\n",
        "        if batch_idx == len(train_loader) - 1:\n",
        "            print(\n",
        "                f\"* Train Epoch: {epoch}-{batch_idx:03d} \\n\"\n",
        "                f\"batch_loss={loss_float:0.2e} \"\n",
        "                f\"batch_acc={accuracy_float:0.3f} \"\n",
        "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \\n\"\n",
        "            )\n",
        "      \"\"\"\n",
        "    epoch_accuracy = np.sum(accuracy_history) / len(train_loader)\n",
        "    epoch_loss = np.sum(loss_history) / len(train_loader)\n",
        "    print(f\"* Train Epoch: {epoch} \\n\"\n",
        "          f\"average accuracy = {100. * epoch_accuracy:.2f}\")\n",
        "\n",
        "    return loss_history, accuracy_history, lr_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkhGb9jJoJUI"
      },
      "source": [
        "## 4. Define the testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AbjxzqtCmiwb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = torch.argmax(output, dim=1).float().clone().detach().requires_grad_(False)\n",
        "            #pred = torch.tensor(torch.argmax(output, dim=1).float(), requires_grad=False)\n",
        "\n",
        "            test_loss += criterion(pred, target)  # sum up batch loss\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = correct / len(test_loader.dataset)\n",
        "    print(f'* Test set: \\n Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * test_accuracy:.2f}%)\\n')\n",
        "    return test_loss, test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2usx0TwAoMp8"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_f1(model, device, test_loader):\n",
        "    model.eval()\n",
        "    batch_f1 = np.zeros(len(test_loader))\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):    # iterate over each minibatch\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        pred = torch.argmax(output, dim=1).float().clone().detach().requires_grad_(False)\n",
        "\n",
        "        batch_f1[batch_idx] = multiclass_f1_score(pred, target, num_classes=2)   # compute the F1 score for 1 minibatch\n",
        "\n",
        "    f1_score = np.sum(batch_f1) / len(test_loader)  # Output is the average of the f1 score over all minibatches\n",
        "\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nerW78SArNee"
      },
      "source": [
        "## 5. Define the training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e7Pala2UrQ_O"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtlUfegVoPLJ"
      },
      "source": [
        "## 6. Train the network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPqgDZtdoSR1"
      },
      "outputs": [],
      "source": [
        "# Training History\n",
        "lr_history = []\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "\n",
        "# Validation History\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "i = 1\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\" === Training starting for epoch {i} / {num_epochs} === \\n\")\n",
        "    train_loss, train_acc, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    train_acc_history.extend(train_acc)\n",
        "    lr_history.extend(lrs)\n",
        "\n",
        "    # Test the model after each epoch\n",
        "    test_loss, test_accuracy = test_model(model, test_loader, criterion, device)\n",
        "    val_loss_history.append(test_loss)\n",
        "    val_acc_history.append(test_accuracy)\n",
        "    i += 1\n",
        "\n",
        "f1_score = compute_f1(model, device, test_loader)\n",
        "\n",
        "print(f\"Final F1 score: {f1_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNnBpZQnoVww"
      },
      "source": [
        "## 6. Analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqObCpC8oZ3R",
        "outputId": "a7d9be2b-0e2d-41b6-cb3a-c2331b5a2946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning rate []\n",
            "train loss []\n",
            "train accuracy []\n",
            "test loss []\n",
            "test accuracy []\n"
          ]
        }
      ],
      "source": [
        "print(\"learning rate\", lr_history)\n",
        "print(\"train loss\", train_loss_history)\n",
        "print(\"train accuracy\", train_acc_history)\n",
        "print(\"test loss\", val_loss_history)\n",
        "print(\"test accuracy\", val_acc_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5t6nDErobk5"
      },
      "outputs": [],
      "source": [
        "n_train = len(train_acc_history)\n",
        "t_train = num_epochs * np.arange(n_train) / n_train\n",
        "t_test = np.arange(1, num_epochs + 1)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plotting learning rate history\n",
        "axs[0].plot(t_train, lr_history)\n",
        "axs[0].set_title('Learning Rate History')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Learning Rate')\n",
        "\n",
        "# Plotting training loss history\n",
        "axs[1].plot(t_train, train_loss_history, color='b', label=\"Train\")\n",
        "#axs[1].plot(t_test, val_loss_history, label=\"Test\", color='orange')\n",
        "axs[1].legend()\n",
        "axs[1].set_title('Training Loss History')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "\n",
        "# Plotting training accuracy history\n",
        "axs[2].plot(t_train, train_acc_history, color='b', label=\"Train\")\n",
        "axs[2].plot(t_test, val_acc_history, label=\"Test\", color='orange')\n",
        "axs[2].legend()\n",
        "axs[2].set_title('Training Accuracy History')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPuVXDUlofyb"
      },
      "source": [
        "## 7. Debugging tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_3sUlV4oiHH"
      },
      "source": [
        "### a) Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SwGurwSolYv"
      },
      "outputs": [],
      "source": [
        "# Convert PyTorch tensors to NumPy arrays\n",
        "model.eval()\n",
        "predictions = []\n",
        "labels = []\n",
        "model = model.to(device)\n",
        "\n",
        "for data, target in test_loader:\n",
        "  data, target = data.to(device), target.to(device)\n",
        "  output = model(data)\n",
        "  pred = torch.argmax(output, dim=1).float().clone().detach().requires_grad_(False)\n",
        "  pred = pred.cpu().numpy()\n",
        "  target = target.cpu().numpy()\n",
        "\n",
        "  predictions.append(pred)\n",
        "  labels.append(target)\n",
        "\n",
        "print(\"predictions: \", predictions)\n",
        "print(\"labels: \", labels)\n",
        "\n",
        "\n",
        "# Compute confusion matrix using scikit-learn\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "# Create a heatmap for visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Class 0\", \"Class 1\"],\n",
        "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqa4tMPooZt"
      },
      "source": [
        "### b) Visualize misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-K4Ef-Vonuw"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "misclassified_images = []\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the test set\n",
        "for data, target in test_loader:\n",
        "    # Forward pass\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    pred = torch.argmax(output, dim=1).float().clone().detach().requires_grad_(False)\n",
        "\n",
        "    # Identify misclassified images\n",
        "    misclassified_mask = pred != target\n",
        "    misclassified_images.extend(data[misclassified_mask])\n",
        "    true_labels.extend(target[misclassified_mask])\n",
        "    predicted_labels.extend(pred[misclassified_mask])\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "misclassified_images = torch.stack(misclassified_images)\n",
        "true_labels = torch.stack(true_labels)\n",
        "predicted_labels = torch.stack(predicted_labels)\n",
        "\n",
        "# Assuming you have a function to reverse normalization if applied during data preprocessing\n",
        "# Replace reverse_normalize with your actual function\n",
        "# Example:\n",
        "reverse_normalize = transforms.Compose([transforms.Normalize(mean=0, std=(1/62.7087)),\n",
        "                                        transforms.Normalize(mean=-51.6644, std=1)])\n",
        "\n",
        "# Display misclassified images\n",
        "num_images_to_display = min(10, len(misclassified_images))\n",
        "fig, axes = plt.subplots(1, num_images_to_display, figsize=(45,6))\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Assuming images are in RGB format\n",
        "    image = reverse_normalize(misclassified_images[i]).cpu().numpy()\n",
        "    true_label = true_labels[i].item()\n",
        "    predicted_label = predicted_labels[i].item()\n",
        "\n",
        "    axes[i].imshow(image[15,:,:], cmap='sdoaia304')\n",
        "    axes[i].set_title(f'True: {true_label}, Predicted: {predicted_label}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
