{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7201277,"sourceType":"datasetVersion","datasetId":4165525}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"IMPORTANT: for this code use data0 and data1 without separating testing data into a separate folder","metadata":{"id":"RvPPmFWrm7hG"}},{"cell_type":"markdown","source":"<div style=\"width: 1350px; height: 30px; background-color: green;\"></div>\n","metadata":{"id":"g0PfonFkm7hG"}},{"cell_type":"markdown","source":"# 0. Load Modules","metadata":{"id":"p3xO2wwt8WI_"}},{"cell_type":"markdown","source":"Convert to py:\n\n`jupyter nbconvert --to script RCNN_crossval.ipynb`","metadata":{"id":"tZ7mpr4b8WI_"}},{"cell_type":"code","source":"# Core Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Operating System Interaction\nimport os\nimport sys\n\n# Machine Learning Frameworks\nimport torch\nfrom torchvision import datasets\nfrom torch.utils.data import Dataset, DataLoader\n\n# Data Transformation and Augmentation (not all of these transformations were finally used)\nfrom torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, \\\n    RandomVerticalFlip, ColorJitter, RandomAffine, RandomPerspective, RandomResizedCrop, \\\n    GaussianBlur, RandomAutocontrast\nfrom torchvision.transforms import functional as F\n\n# Model Building and Initialization\nimport torch.nn as nn\nfrom torch.nn.init import kaiming_normal_\n\n# Data Loading and Dataset Handling\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import random_split, Subset\nfrom PIL import Image\n\n# Cross-Validation and Metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, roc_curve, auc, accuracy_score\nfrom scipy.special import expit as sigmoid\n\n# Visualization and Display\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.colors import Normalize\nfrom IPython.display import HTML\n\n# Miscellaneous\nimport random\nfrom tqdm import tqdm","metadata":{"id":"dvMqFAyy8WJA","execution":{"iopub.status.busy":"2023-12-14T17:42:41.465193Z","iopub.execute_input":"2023-12-14T17:42:41.466005Z","iopub.status.idle":"2023-12-14T17:42:47.958984Z","shell.execute_reply.started":"2023-12-14T17:42:41.465970Z","shell.execute_reply":"2023-12-14T17:42:47.958188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For Google Colab, mount Google Drive, for local environments, get local path (github)\n\n# Change with the appropriate path. Log in into Drive and create the folders with the data\n\nif 'google.colab' in sys.modules:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    # Carlos\n    folder0_path = '/content/drive/My Drive/solar_jets/data0'\n    folder1_path = '/content/drive/My Drive/solar_jets/data1'\nelse:\n    # For local environments like VS Code\n    #folder0_path = './data_all/data0'\n    #folder1_path = './data_all/data1'\n\n    # Kaggle\n    folder0_path = '/kaggle/input/solar-jets/data0/data0'\n    folder1_path = '/kaggle/input/solar-jets/data1/data1'","metadata":{"id":"tTmZUgya8WJB","outputId":"573aa256-a204-48f9-b12e-210a0e045fe3","execution":{"iopub.status.busy":"2023-12-14T17:42:47.960754Z","iopub.execute_input":"2023-12-14T17:42:47.961169Z","iopub.status.idle":"2023-12-14T17:42:47.966293Z","shell.execute_reply.started":"2023-12-14T17:42:47.961136Z","shell.execute_reply":"2023-12-14T17:42:47.965443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Prepare the dataset","metadata":{"id":"bcGqScbn8WJC"}},{"cell_type":"markdown","source":"#### Create the class","metadata":{"id":"sSF0xLC78WJC"}},{"cell_type":"code","source":"class TensorTransforms:\n    def __init__(self, rotate_angle=30):\n        self.rotate_angle = rotate_angle\n\n    def random_horizontal_flip(self, x):\n        if random.random() > 0.5:\n            return torch.flip(x, [2])  # Flip along width\n        return x\n\n    def random_vertical_flip(self, x):\n        if random.random() > 0.5:\n            return torch.flip(x, [1])  # Flip along height\n        return x\n\n    def random_rotation(self, x):\n        # Random rotation in increments of 90 degrees for simplicity\n        k = random.randint(0, 3)  # 0, 90, 180, or 270 degrees\n        return torch.rot90(x, k, [1, 2])  # Rotate along height and width\n\n    def __call__(self, x):\n        x = self.random_horizontal_flip(x)\n        x = self.random_vertical_flip(x)\n        x = self.random_rotation(x)\n        return x\n\nclass NPZDataset(Dataset):\n    def __init__(self, data_dir, augment=True, mean=None, std=None):\n        self.data_dir = data_dir\n        self.augment = augment\n        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n        self.transform = TensorTransforms()\n        self.mean = mean\n        self.std = std\n\n    def _check_file_shape(self, file):\n        file_path = os.path.join(self.data_dir, file)\n        data = np.load(file_path)['arr_0']\n        return data.shape == (166, 166, 30)\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        file_path = os.path.join(self.data_dir, self.files[idx])\n        data = np.load(file_path)['arr_0']\n        data = np.moveaxis(data, -1, 0)  # Move channel to first dimension\n\n        data = torch.from_numpy(data).float()  # Convert to PyTorch tensor\n\n        if self.augment:\n            data = self.transform(data)\n\n        if self.mean is not None and self.std is not None:\n            data = np.clip(data, a_min=None, a_max= 1000)\n            data = (data - self.mean) / self.std\n\n\n        label = 1.0 if 'data1' in self.data_dir else 0.0\n\n        return data, np.float32(label)","metadata":{"id":"xB4lQoqum7hQ","execution":{"iopub.status.busy":"2023-12-14T17:42:47.967459Z","iopub.execute_input":"2023-12-14T17:42:47.967756Z","iopub.status.idle":"2023-12-14T17:42:47.981529Z","shell.execute_reply.started":"2023-12-14T17:42:47.967732Z","shell.execute_reply":"2023-12-14T17:42:47.980663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run to get the Data","metadata":{"id":"pvVU4cLZ8WJC"}},{"cell_type":"markdown","source":"We calculated the mean and std previously with function compute_mean_std","metadata":{"id":"IR5Cg0Mm5OX1"}},{"cell_type":"code","source":"mean_data = 50.564544677734375\nstd_data = 49.94772720336914","metadata":{"id":"ofvsSor18WJD","execution":{"iopub.status.busy":"2023-12-14T17:42:47.982610Z","iopub.execute_input":"2023-12-14T17:42:47.983385Z","iopub.status.idle":"2023-12-14T17:42:47.997909Z","shell.execute_reply.started":"2023-12-14T17:42:47.983360Z","shell.execute_reply":"2023-12-14T17:42:47.997118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data1 = NPZDataset(folder1_path, mean=mean_data, std=std_data, augment=True)\ntrain_data0 = NPZDataset(folder0_path, mean=mean_data, std=std_data, augment=True)\ntrain_data = torch.utils.data.ConcatDataset([train_data1, train_data0])","metadata":{"id":"55A8A8oBm7hS","execution":{"iopub.status.busy":"2023-12-14T17:42:48.000131Z","iopub.execute_input":"2023-12-14T17:42:48.000387Z","iopub.status.idle":"2023-12-14T17:43:41.483866Z","shell.execute_reply.started":"2023-12-14T17:42:48.000365Z","shell.execute_reply":"2023-12-14T17:43:41.483042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just for debugging\n# train_data1 = ([train_data1[i] for i in range(100)])\n# train_data0 = ([train_data0[i] for i in range(100)])\n# train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])","metadata":{"id":"zgiQUsyzEtsE","execution":{"iopub.status.busy":"2023-12-14T17:43:41.485035Z","iopub.execute_input":"2023-12-14T17:43:41.485385Z","iopub.status.idle":"2023-12-14T17:43:41.489786Z","shell.execute_reply.started":"2023-12-14T17:43:41.485353Z","shell.execute_reply":"2023-12-14T17:43:41.488839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we'll see how train_data can be treated just like lists\n\n- One element in the list for each sequence of images\n- Each element of the list has a tuple of two elements, the first the array (166,30,30) and the second the label","metadata":{"id":"hUpUkTw48WJD"}},{"cell_type":"code","source":"print(\"samples y=1: \",len(train_data1))\nprint(\"samples y=0: \",len(train_data0))\nprint(\"samples total \",len(train_data))","metadata":{"id":"h-oXlEEi8WJD","outputId":"51260d67-a601-4f4b-9c48-a70c54f5120d","execution":{"iopub.status.busy":"2023-12-14T17:43:41.490915Z","iopub.execute_input":"2023-12-14T17:43:41.491225Z","iopub.status.idle":"2023-12-14T17:43:41.503293Z","shell.execute_reply.started":"2023-12-14T17:43:41.491194Z","shell.execute_reply":"2023-12-14T17:43:41.502467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comput mean and std for train dataset (do not run this)","metadata":{"id":"ooTweIk78WJF"}},{"cell_type":"code","source":"def compute_mean_std(dataset):\n    mean = 0.0\n    std = 0.0\n    for data, _ in dataset:\n        mean += data.mean()\n        std += data.std()\n    mean /= len(dataset)\n    std /= len(dataset)\n    return mean.item(), std.item()\n\n# mean, std = compute_mean_std(train_data)\n# mean, std","metadata":{"id":"fZwNPOkq8WJF","execution":{"iopub.status.busy":"2023-12-14T17:43:41.504342Z","iopub.execute_input":"2023-12-14T17:43:41.504587Z","iopub.status.idle":"2023-12-14T17:43:41.518313Z","shell.execute_reply.started":"2023-12-14T17:43:41.504566Z","shell.execute_reply":"2023-12-14T17:43:41.517638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Divide train, validate, and test data","metadata":{"id":"gNeiq6A18WJF"}},{"cell_type":"code","source":"train_size = int(0.7 * len(train_data))  # 70% of data for training\nvalidate_size = int(0.15 * len(train_data))  # 15% for validation\ntest_size = len(train_data) - train_size - validate_size  # remaining 15% for testing\n\ntrain_dataset, validate_dataset, test_dataset = random_split(train_data, [train_size, validate_size, test_size])","metadata":{"id":"U_vh2PCW8WJF","execution":{"iopub.status.busy":"2023-12-14T17:43:41.519436Z","iopub.execute_input":"2023-12-14T17:43:41.519924Z","iopub.status.idle":"2023-12-14T17:43:41.562849Z","shell.execute_reply.started":"2023-12-14T17:43:41.519894Z","shell.execute_reply":"2023-12-14T17:43:41.562168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train: \",len(train_dataset))\nprint(\"validate: \",len(validate_dataset))\nprint(\"test: \",len(test_dataset))","metadata":{"id":"ISiAU_sos-kt","outputId":"6d859288-8ce0-4d95-ada4-daa1298c7a86","execution":{"iopub.status.busy":"2023-12-14T17:43:41.563783Z","iopub.execute_input":"2023-12-14T17:43:41.564009Z","iopub.status.idle":"2023-12-14T17:43:41.569002Z","shell.execute_reply.started":"2023-12-14T17:43:41.563989Z","shell.execute_reply":"2023-12-14T17:43:41.568168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 12\nyour_array = train_data[index][0].numpy()\nprint(train_data[index][1])\n\n# Function to update the plot\ndef update(frame):\n    im.set_array(your_array[frame])\n    return [im]\n\n# Set up the plot\nfig, ax = plt.subplots()\nnorm = Normalize(vmin=your_array.min(), vmax=your_array.max())  # Adjust as needed\nim = ax.imshow(your_array[0], cmap='hot', norm=norm)  # Initialize with the first frame\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=range(30), blit=True)\n\n# Display as HTML5 video in the notebook\nHTML(ani.to_jshtml())","metadata":{"id":"iBlyxujhm7hX","outputId":"db952dc4-1190-4985-ffd0-27b066c975de","execution":{"iopub.status.busy":"2023-12-14T17:43:41.570040Z","iopub.execute_input":"2023-12-14T17:43:41.570396Z","iopub.status.idle":"2023-12-14T17:43:45.714419Z","shell.execute_reply.started":"2023-12-14T17:43:41.570364Z","shell.execute_reply":"2023-12-14T17:43:45.713437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DataLoader is designed to iterate over batches of data rather than individual samples, so when we try to access for example the first mini-batch as `train_loader[0]`, we get an error.\n\nHowever, note how before we could iterate over it\n","metadata":{"id":"DYtnaQCp8WJG"}},{"cell_type":"markdown","source":"# 2. Define the Neural Network","metadata":{"id":"xjoFf4yO8WJH"}},{"cell_type":"code","source":"class RCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(30,64,kernel_size=3,padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64,64,kernel_size=3,padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64,128,kernel_size=3,padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True)\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128,128,kernel_size=3,padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True)\n        )\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(128,256,kernel_size=3,padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(256,256,kernel_size=3,padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(256,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True)\n        )\n        self.layer8 = nn.Sequential(\n            nn.Conv2d(512,512,kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True)\n        )\n        # hidden_size is an hyperparameter to be adjusted\n        # try augmenting num_layers\n        self.lstm = nn.LSTM(input_size=12800, hidden_size=256, num_layers=1, batch_first=True)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(256,128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(128,32),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(32,1),\n            #nn.Sigmoid()     # don't include it as it is already included in BCELogitLoss (BCELoss is less stable)\n        )\n\n        # for m in self.modules():\n        #     if not RCNN:\n        #         kaiming_normal_(m.weight,nonlinearity=\"relu\")#Kaiming to initialize the weights\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.max_pool(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.max_pool(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = self.max_pool(out)\n        out = self.layer7(out)\n        out = self.layer8(out)\n        out = self.max_pool(out)\n        out = self.layer8(out)\n        out = self.layer8(out)\n        out = self.max_pool(out)\n        out = out.view(out.size(0),-1)\n        lstm_out, _ = self.lstm(out)\n        out = self.classifier(lstm_out)\n\n        return out\n\n    def graph(self): #for visualization and debugging\n        return nn.Sequential(self.layer1,self.layer2,self.maxPool,self.layer3,self.layer4,self.maxPool,self.layer5,self.layer6,self.maxPool,self.layer7,self.layer8, self.maxPool,self.layer8,self.layer8,self.maxPool,self.classifier)","metadata":{"id":"tY5U1rgA8WJH","execution":{"iopub.status.busy":"2023-12-14T17:43:45.715719Z","iopub.execute_input":"2023-12-14T17:43:45.716075Z","iopub.status.idle":"2023-12-14T17:43:45.735102Z","shell.execute_reply.started":"2023-12-14T17:43:45.716041Z","shell.execute_reply":"2023-12-14T17:43:45.734055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training Parameters","metadata":{"id":"qyB-i0kb8WJI"}},{"cell_type":"code","source":"def plot_lr(epochs, lrs):\n    # Creating subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n\n    # Linear scale plot\n    ax1.plot(epochs, lrs, marker='o')\n    ax1.set_title('Learning Rate per Epoch (Linear Scale)')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Learning Rate')\n    ax1.set_xticks(epochs)\n    ax1.grid(True)\n\n    # Log scale plot\n    ax2.plot(epochs, lrs, marker='o')\n    ax2.set_title('Learning Rate per Epoch (Log Scale)')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Learning Rate')\n    ax2.set_yscale('log')\n    ax2.set_xticks(epochs)\n    ax2.grid(True)\n\n    # Display the plots\n    plt.show()","metadata":{"id":"eMUYYp7fm7hg","execution":{"iopub.status.busy":"2023-12-14T17:43:45.736436Z","iopub.execute_input":"2023-12-14T17:43:45.736852Z","iopub.status.idle":"2023-12-14T17:43:45.754984Z","shell.execute_reply.started":"2023-12-14T17:43:45.736816Z","shell.execute_reply":"2023-12-14T17:43:45.754278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nbatch_size = 2\n\ninitial_lr = 1e-4\nweight_decay = 5e-3\n\nthreshold = 0.5\n\nmodel = RCNN()\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Function to create optimizer and scheduler\ndef reset_optimizer_scheduler():\n    optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-10, last_epoch=-1)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=3, factor=0.75)\n\n    return optimizer, scheduler\n\n# Initialize optimizer and scheduler\noptimizer, scheduler = reset_optimizer_scheduler()\ncriterion = torch.nn.BCEWithLogitsLoss()\n\n# Lists to store epoch numbers and corresponding learning rates\nepochs = []\nlrs = []\n\n# Loop to update and record learning rate\nfor epoch in range(num_epochs):\n    optimizer.step()\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    epochs.append(epoch + 1)\n    lrs.append(current_lr)\n\nplot_lr(epochs, lrs)","metadata":{"id":"_tquXPNJm7hr","outputId":"729d7b58-e1c1-4889-8590-745e02c5f785","execution":{"iopub.status.busy":"2023-12-14T17:45:02.370356Z","iopub.execute_input":"2023-12-14T17:45:02.371050Z","iopub.status.idle":"2023-12-14T17:45:04.689628Z","shell.execute_reply.started":"2023-12-14T17:45:02.371019Z","shell.execute_reply":"2023-12-14T17:45:04.688612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Train Epoch Script","metadata":{"id":"IUqXkC0g8WJJ"}},{"cell_type":"code","source":"def train_epoch(model, optimizer, scheduler, criterion, train_loader, device, threshold):\n    model.train()\n    correct, train_loss, total_length = 0, 0, 0\n\n    #for i, (data, target) in tqdm(enumerate(train_loader),desc=\"Epoch no.\"+str(epoch)):\n    for i, (data, target) in enumerate(train_loader):\n\n        #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n        data, target = data.to(device), target.to(device).unsqueeze(1).float()\n\n        #FORWARD PASS\n        output = model(data)\n        loss = criterion(output, target)\n\n        #BACKWARD AND OPTIMIZE\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # PREDICTIONS\n        threshold = 0.5\n        pred = (torch.sigmoid(output) >= threshold).float()\n\n        # PERFORMANCE CALCULATION\n        train_loss += loss.item() * len(data)\n        total_length += len(data)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\n    scheduler.step()\n    train_loss = train_loss / total_length\n    train_acc = correct / total_length\n\n    return train_loss, train_acc, scheduler.get_last_lr()[0]","metadata":{"id":"urR48DBX8WJK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Test Script","metadata":{"id":"7uv1vX6J8WJM"}},{"cell_type":"code","source":"def predict(model, train_loader, criterion, device, threshold):\n    model.eval()\n    correct, val_loss, total_length = 0, 0, 0\n    all_preds = []\n    all_targets = []\n\n    with torch.no_grad():\n        for data, target in train_loader:\n\n            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n\n            #FORWARD PASS\n            output = model(data)\n            loss = criterion(output, target)\n            # PREDICTIONS\n            threshold = 0.5\n            pred = (torch.sigmoid(output) >= threshold).float()\n\n            # PERFORMANCE CALCULATION\n            val_loss += loss.item() * len(data)\n            total_length += len(data)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            all_preds.extend(pred.view(-1).cpu().numpy())\n            all_targets.extend(target.view(-1).cpu().numpy())\n\n    val_loss = val_loss / total_length\n    val_acc = correct / total_length\n\n    return val_loss, val_acc, np.array(all_preds), np.array(all_targets)\n\n\ndef test(model, train_loader, criterion, device, threshold):\n    model.eval()\n    correct, val_loss, total_length = 0, 0, 0\n    all_preds = []\n    all_targets = []\n    all_out = []\n\n    with torch.no_grad():\n        for data, target in train_loader:\n\n            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n\n            #FORWARD PASS\n            output = model(data)\n            loss = criterion(output, target)\n            # PREDICTIONS\n            threshold = 0.5\n            pred = (torch.sigmoid(output) >= threshold).float()\n\n            # PERFORMANCE CALCULATION\n            val_loss += loss.item() * len(data)\n            total_length += len(data)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            all_preds.extend(pred.view(-1).cpu().numpy())\n            all_targets.extend(target.view(-1).cpu().numpy())\n            all_out.extend(output.view(-1).cpu().numpy())\n\n    val_loss = val_loss / total_length\n    val_acc = correct / total_length\n\n    return val_loss, val_acc, np.array(all_preds), np.array(all_targets), np.array(all_out)","metadata":{"id":"38S9p-Y88WJN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Train the Network ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€\n","metadata":{"id":"0yKfGhWj8WJP"}},{"cell_type":"code","source":"# Model\nmodel = RCNN() #creates an instance of the RCNN\n\n# Loading in consequence\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# Hyperparameters\n\noptimizer, scheduler = reset_optimizer_scheduler()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n# Logs results\nloss_history_train=[]\nlr_history_train=[]\nacc_history_train=[]\n\nacc_history_test=[]\nloss_history_test=[]\n\nf1_val_history = []\n\n\n# Early stopping parameters\npatience = 101\nbest_loss = float('inf')\nepochs_no_improve = 0\n\n# Save best f1 model\nbest_model_state = None\nbest_f1_score = 0\n\n\n# RUN MODEL\n\nfor epoch in range(1,num_epochs+1):\n\n  # Train\n  train_loss, train_acc, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, device, threshold)\n  loss_history_train.append(train_loss)\n  acc_history_train.append(train_acc)\n  lr_history_train.append(lrs)\n\n  # Validate\n  val_loss, val_acc, val_preds, val_targets = predict(model, valid_loader, criterion, device, threshold)\n  loss_history_test.append(val_loss)\n  acc_history_test.append(val_acc)\n\n  # Calculate F1 score\n  f1 = f1_score(val_targets, val_preds, average='binary')  # adjust the average parameter as needed\n  f1_val_history.append(f1)\n\n  print(f\"Epoch {epoch}/{num_epochs} - LR: {lrs:.3e}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val F1 Score: {f1:.4f}\")\n\n  # Early stopping\n  if val_loss < best_loss:\n    best_loss = val_loss\n    epochs_no_improve = 0\n  else:\n    epochs_no_improve += 1\n\n  if epochs_no_improve == patience:\n    print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n    break\n\n  # Save Model with best F1\n  if f1 == max(f1_val_history):\n      best_f1_score = f1\n      best_model_state = model.state_dict()  # Save the model state","metadata":{"id":"n5rrUD8WyjWe","outputId":"5e4fc7e1-6813-486a-e8ba-977247c4f6e2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on TEST data\n#test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=train_subsampler)\n\nmodel.load_state_dict(best_model_state)\nmodel.to(device)\n\ntest_loss, test_accuracy, test_predictions, test_targets, test_out = test(model, test_loader, criterion, device, threshold)\ntest_f1_score = f1_score(test_targets, test_predictions, average='binary')  # adjust as needed\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1_score:.4f}\")","metadata":{"id":"pZZ2_one94mo","outputId":"94c3ea5b-c001-4d76-fa59-530d9a79076f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_history=acc_history_test.copy()\nloss_history=loss_history_test\ntrain_loss = loss_history_train.copy()\n\n# Use numpy's convolve function to calculate the moving average\nmoving_avg_train = np.convolve(train_loss, np.ones(100)/100, mode='valid')\n#print(len(train_loss))\n#print(len(moving_avg_train))\n\nn_train = len(accuracy_history)\n\nt_test = np.arange(1, num_epochs + 1)\nt_train = np.linspace(1,num_epochs,len(loss_history_train))\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 5))\n\n# Plotting training loss history\naxs[0].semilogy(t_train, train_loss, color='orange', label=\"training loss\")\n#axs[0].semilogy((t_train[:-99]), moving_avg_train, color='blue', label=\"100-values moving avg\")\naxs[0].legend()\naxs[0].set_title('Train loss History')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Loss')\n\naxs[1].plot(t_test, accuracy_history, label=\"Validation\", color='orange')\naxs[1].set_title('Validation Accuracy History')\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\n\naxs[2].plot(t_test, lr_history_train)\naxs[2].set_title('Learning Rate History')\naxs[2].set_xlabel('Epoch')\naxs[2].set_ylabel('Learning Rate')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"9ep-wtC_FVAs","outputId":"beaa0a27-6fa1-480e-c2eb-abe17f995e74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute ROC curve and ROC area for each class\nfpr, tpr, _ = roc_curve(test_targets, test_out)\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Area Under the Curve (AUC)\nprint(\"Area Under the Curve (AUC): \", roc_auc)","metadata":{"id":"vH1pJNUbm7hx","outputId":"f2e76325-f5b1-44cd-afc8-72f28a8f5749","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# F1 vs threshold\nthresholds = np.linspace(0.03, 0.97, 100)  # 100 thresholds between 0 and 1\nf1_scores = []\naccuracy_scores = []\n\nprobabilities = sigmoid(test_out)\n\nfor thresh in thresholds:\n    # Convert probabilities to binary predictions based on the threshold\n    predictions = (probabilities > thresh).astype(int)\n\n    # Compute F1 score and Accuracy for this threshold\n    f1_scores.append(f1_score(test_targets, predictions))\n    accuracy_scores.append(accuracy_score(test_targets, predictions))\n\n\n\nplt.figure(figsize=(12, 5))\n\n# F1 Score subplot\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, f1_scores, marker='')\nplt.title('F1 Score vs. Thresholds')\nplt.xlabel('Threshold')\nplt.ylabel('F1 Score')\nplt.grid()\n\n# Accuracy Score subplot\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, accuracy_scores, marker='', color='green')\nplt.title('Accuracy vs. Thresholds')\nplt.xlabel('Threshold')\nplt.ylabel('Accuracy')\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n# As data is balanced, f1 and accuracy are similar","metadata":{"id":"TrKJ4Wj7PeKJ","outputId":"4732f700-5f6e-45f7-ac2f-088257b7cfe8","trusted":true},"execution_count":null,"outputs":[]}]}