{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import pandas as pd\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 500, 500, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.load(\"./data_1.npy\")\n",
    "data.shape #(num_event, 500,500, num_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"15_first_events.csv\")\n",
    "\n",
    "\n",
    "#maybe we will need to use that\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),           # Convert to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize if needed\n",
    "])\n",
    "#Specify the path to your custom dataset\n",
    "custom_dataset_path = '/path/to/your/dataset' #normally ./data/0/ and ./data/1/\n",
    "\n",
    "# Load the custom dataset using ImageFolder\n",
    "custom_dataset = ImageFolder(root=custom_dataset_path, transform=transform)\n",
    "\n",
    "# Create a DataLoader for iterating over batches\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Iterate over batches\n",
    "for images, labels in custom_dataloader:\n",
    "    # Your custom processing or training logic here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "        \n",
    "        self.input_layer=torch.nn.Linear(in_features=500*500*15, out_features=500*500*15)\n",
    "\n",
    "        self.norm=torch.nn.BatchNorm1d()#not sure this is sufficent for batch renormalization\n",
    "\n",
    "        #Three convolutionnal and batch renormalization\n",
    "        self.conv1=torch.nn.Conv1d(in_channels=15, out_channels=64, kernel_size=3, stride=1) #need padding of one normally but not precised\n",
    "        self.relu1=torch.nn.ReLU()\n",
    "        self.norm1=torch.nn.BatchNorm1d()\n",
    "\n",
    "        self.conv2=torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu2=torch.nn.ReLU()\n",
    "        self.norm2=torch.nn.BatchNorm1d()\n",
    "\n",
    "        self.conv3=torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu3=torch.nn.ReLU()\n",
    "        self.norm3=torch.nn.BatchNorm1d()\n",
    "\n",
    "        #First max pooling\n",
    "        self.max1=torch.nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "\n",
    "        #Three convolutionnal and batch renormalization\n",
    "        self.conv4=torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu4=torch.nn.ReLU()\n",
    "        self.norm4=torch.nn.BatchNorm1d()\n",
    "\n",
    "        self.conv5=torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu5=torch.nn.ReLU()\n",
    "        self.norm5=torch.nn.BatchNorm1d()\n",
    "\n",
    "        self.conv6=torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu6=torch.nn.ReLU()\n",
    "        self.norm6=torch.nn.BatchNorm1d()\n",
    "\n",
    "        #Second maxpool\n",
    "        self.max2=torch.nn.MaxPool1d(stride=2,kernel_size=2)\n",
    "\n",
    "        #Flatten\n",
    "        self.flat1=torch.nn.Flatten()\n",
    "\n",
    "        #fully connected with dropout\n",
    "        self.lin=torch.nn.Linear(in_features=2304, out_features=1024)\n",
    "        self.drop=torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        #LSTM\n",
    "        self.lstm=torch.nn.LSTM(input_size=1024,output_size=512, num_layers=1)\n",
    "\n",
    "        #output\n",
    "        self.output_layer=torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.input_layer(x)\n",
    "        x=self.norm(x)#not sure this is sufficent for batch renormalization\n",
    "\n",
    "        #Three convolutionnal and batch renormalization\n",
    "        x=self.conv1(x) \n",
    "        x=self.relu1(x)\n",
    "        x=self.norm1(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.norm2(x)\n",
    "\n",
    "        x=self.conv3(x)\n",
    "        x=self.relu3(x)\n",
    "        x=self.norm3(x)\n",
    "\n",
    "        #First max pooling\n",
    "        x=self.max1(x)\n",
    "\n",
    "        #Three convolutionnal and batch renormalization\n",
    "        x=self.conv4(x)\n",
    "        x=self.relu4(x)\n",
    "        x=self.norm4(x)\n",
    "\n",
    "        x=self.conv5(x)\n",
    "        x=self.relu5(x)\n",
    "        x=self.norm5(x)\n",
    "\n",
    "        x=self.conv6(x)\n",
    "        x=self.relu6(x)\n",
    "        x=self.norm6(x)\n",
    "\n",
    "        #Second maxpool\n",
    "        x=self.max2(x)\n",
    "\n",
    "        #Flatten\n",
    "        x=self.flat1(x)\n",
    "\n",
    "        #fully connected with dropout\n",
    "        x=self.lin(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        #LSTM\n",
    "        x=self.lstm(x)\n",
    "\n",
    "        #output\n",
    "        x=self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=torch.nn.Sequential(\n",
    "\n",
    "    torch.nn.Linear(in_features=500*500*15, out_features=500*500*15),\n",
    "\n",
    "    torch.nn.BatchNorm1d(),#not sure this is sufficent for batch renormalization-->\n",
    "\n",
    "        #Three convolutionnal and batch renormalization\n",
    "    torch.nn.Conv1d(in_channels=15, out_channels=64, kernel_size=3, stride=1), #need padding of one normally but not precised\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(),\n",
    "\n",
    "    torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(),\n",
    "\n",
    "    torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(),\n",
    "\n",
    "        #First max pooling\n",
    "    torch.nn.MaxPool1d(kernel_size=2,stride=2),\n",
    "\n",
    "        #Three convolutionnal and batch renormalization\n",
    "    torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(),\n",
    "\n",
    "    torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(),\n",
    "\n",
    "    torch.nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(),\n",
    "\n",
    "        #Second maxpool\n",
    "    torch.nn.MaxPool1d(stride=2,kernel_size=2),\n",
    "\n",
    "        #Flatten\n",
    "    torch.nn.Flatten(),\n",
    "\n",
    "        #fully connected with dropout\n",
    "    torch.nn.Linear(in_features=2304, out_features=1024),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        #LSTM\n",
    "    torch.nn.LSTM(input_size=1024,output_size=512, num_layers=1),\n",
    "\n",
    "        #output\n",
    "    torch.nn.Softmax(dim=1),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an exemple of batch renormalization\n",
    "class BatchRenorm2d(torch.nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, r_d_max_inc_step=0.0001):\n",
    "        super(BatchRenorm2d, self).__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.r_d_max_inc_step = r_d_max_inc_step\n",
    "\n",
    "        # Parameters\n",
    "        self.weight = torch.nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
    "        self.running_mean = torch.nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
    "        self.running_var = torch.nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
    "        self.gamma = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "        self.beta = torch.nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "        # Batch renormalization parameters\n",
    "        self.r_max = torch.nn.Parameter(torch.Tensor([3.0]))\n",
    "        self.d_max = torch.nn.Parameter(torch.Tensor([5.0]))\n",
    "        self.c_max = torch.nn.Parameter(torch.Tensor([5.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=[0, 2, 3], keepdim=True)\n",
    "            var = x.var(dim=[0, 2, 3], unbiased=False, keepdim=True)\n",
    "\n",
    "            self.running_mean.mul_(1 - self.momentum)\n",
    "            self.running_var.mul_(1 - self.momentum)\n",
    "            self.running_mean.add_(mean * self.momentum)\n",
    "            self.running_var.add_(var * self.momentum)\n",
    "\n",
    "            r_d_max_step = self.r_d_max_inc_step * (self.r_max / 3.0)\n",
    "            r_max = torch.clamp(self.r_max + r_d_max_step, min=1.0, max=3.0)\n",
    "            d_max = torch.clamp(self.d_max + r_d_max_step, min=0.0, max=5.0)\n",
    "\n",
    "            x = torch.nn.functional.batch_norm(\n",
    "                x, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                training=True, eps=self.eps\n",
    "            )\n",
    "            x = x * self.gamma + self.beta\n",
    "            x = torch.nn.functional.batch_renorm(\n",
    "                x, self.running_mean, self.running_var,\n",
    "                weight=self.weight, bias=self.bias,\n",
    "                running_mean=None, running_var=None,\n",
    "                momentum=self.momentum, eps=self.eps,\n",
    "                rmax=r_max.item(), dmax=d_max.item(), cmax=self.c_max.item()\n",
    "            )\n",
    "        else:\n",
    "            x = torch.nn.functional.batch_norm(\n",
    "                x, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                training=False, eps=self.eps\n",
    "            )\n",
    "            x = x * self.gamma + self.beta\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    @param model: torch.nn.Module\n",
    "    @param criterion: torch.nn.modules.loss._Loss\n",
    "    @param dataset_train: torch.utils.data.DataLoader\n",
    "    @param dataset_test: torch.utils.data.DataLoader\n",
    "    @param optimizer: torch.optim.Optimizer\n",
    "    @param num_epochs: int\n",
    "    \"\"\"\n",
    "    print(\"Starting training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train an epoch\n",
    "        model.train()\n",
    "        for batch_x, batch_y in dataset_train:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        \n",
    "            # Evaluate the network (forward pass)\n",
    "            prediction = model(batch_x)\n",
    "            loss = criterion(prediction, batch_y)\n",
    "\n",
    "            # Compute the gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model with a gradient step\n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        # Test the quality on the test set\n",
    "        model.eval()\n",
    "        accuracies_test = []\n",
    "        for batch_x, batch_y in dataset_test:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            prediction = model(batch_x)\n",
    "            accuracies_test.append(accuracy(prediction, batch_y))\n",
    "\n",
    "        print(\n",
    "            \"Epoch {} | Test accuracy: {:.5f}\".format(\n",
    "                epoch, sum(accuracies_test).item() / len(accuracies_test)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "model=RCNN()\n",
    "criterion = (\n",
    "    torch.nn.CrossEntropyLoss()\n",
    ")  # this includes LogSoftmax which executes a logistic transformation\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envADA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
