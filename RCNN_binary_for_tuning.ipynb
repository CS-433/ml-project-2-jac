{"cells":[{"cell_type":"markdown","metadata":{"id":"g0PfonFkm7hG"},"source":["<div style=\"width: 1350px; height: 30px; background-color: green;\"></div>\n"]},{"cell_type":"markdown","metadata":{"id":"p3xO2wwt8WI_"},"source":["# 0. Load Modules"]},{"cell_type":"markdown","metadata":{"id":"tZ7mpr4b8WI_"},"source":["Convert to py:\n","\n","`jupyter nbconvert --to script RCNN_crossval.ipynb`"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:41.466005Z","iopub.status.busy":"2023-12-14T17:42:41.465193Z","iopub.status.idle":"2023-12-14T17:42:47.958984Z","shell.execute_reply":"2023-12-14T17:42:47.958188Z","shell.execute_reply.started":"2023-12-14T17:42:41.465970Z"},"id":"dvMqFAyy8WJA","trusted":true},"outputs":[],"source":["# Core Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Operating System Interaction\n","import os\n","import sys\n","\n","# Machine Learning Frameworks\n","import torch\n","from torchvision import datasets\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Data Transformation and Augmentation (not all of these transformations were finally used)\n","from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, \\\n","    RandomVerticalFlip, ColorJitter, RandomAffine, RandomPerspective, RandomResizedCrop, \\\n","    GaussianBlur, RandomAutocontrast\n","from torchvision.transforms import functional as F\n","\n","# Model Building and Initialization\n","import torch.nn as nn\n","from torch.nn.init import kaiming_normal_\n","\n","# Data Loading and Dataset Handling\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import random_split, Subset\n","from PIL import Image\n","\n","# Cross-Validation and Metrics\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, roc_curve, auc, accuracy_score\n","from scipy.special import expit as sigmoid\n","\n","# Visualization and Display\n","from matplotlib.animation import FuncAnimation\n","from matplotlib.colors import Normalize\n","from IPython.display import HTML\n","\n","# Miscellaneous\n","import random\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:47.961169Z","iopub.status.busy":"2023-12-14T17:42:47.960754Z","iopub.status.idle":"2023-12-14T17:42:47.966293Z","shell.execute_reply":"2023-12-14T17:42:47.965443Z","shell.execute_reply.started":"2023-12-14T17:42:47.961136Z"},"id":"tTmZUgya8WJB","outputId":"573aa256-a204-48f9-b12e-210a0e045fe3","trusted":true},"outputs":[],"source":["# For Google Colab, mount Google Drive, for local environments, get local path (github)\n","\n","# Change with the appropriate path. Log in into Drive and create the folders with the data\n","\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Carlos\n","    folder0_path = '/content/drive/My Drive/solar_jets/data0'\n","    folder1_path = '/content/drive/My Drive/solar_jets/data1'\n","else:\n","    # For local environments like VS Code\n","    folder0_path = './data0'\n","    folder1_path = './data1'\n","    folder0_test_path = './data0_test'\n","    folder1_test_path = './data1_test'\n","    folder0_valid_path = './data0_val'\n","    folder1_valid_path = './data1_val'\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bcGqScbn8WJC"},"source":["## 1.1. Prepare the dataset"]},{"cell_type":"markdown","metadata":{"id":"sSF0xLC78WJC"},"source":["#### Create the class"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:47.967756Z","iopub.status.busy":"2023-12-14T17:42:47.967459Z","iopub.status.idle":"2023-12-14T17:42:47.981529Z","shell.execute_reply":"2023-12-14T17:42:47.980663Z","shell.execute_reply.started":"2023-12-14T17:42:47.967732Z"},"id":"xB4lQoqum7hQ","trusted":true},"outputs":[],"source":["class TensorTransforms:\n","    def __init__(self, rotate_angle=30):\n","        self.rotate_angle = rotate_angle\n","\n","    def random_horizontal_flip(self, x):\n","        if random.random() > 0.5:\n","            return torch.flip(x, [2])  # Flip along width\n","        return x\n","\n","    def random_vertical_flip(self, x):\n","        if random.random() > 0.5:\n","            return torch.flip(x, [1])  # Flip along height\n","        return x\n","\n","    def random_rotation(self, x):\n","        # Random rotation in increments of 90 degrees for simplicity\n","        k = random.randint(0, 3)  # 0, 90, 180, or 270 degrees\n","        return torch.rot90(x, k, [1, 2])  # Rotate along height and width\n","\n","    def __call__(self, x):\n","        x = self.random_horizontal_flip(x)\n","        x = self.random_vertical_flip(x)\n","        x = self.random_rotation(x)\n","        return x\n","\n","class NPZDataset(Dataset):\n","    def __init__(self, data_dir, augment=True, mean=None, std=None):\n","        self.data_dir = data_dir\n","        self.augment = augment\n","        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n","        self.transform = TensorTransforms()\n","        self.mean = mean\n","        self.std = std\n","\n","    def _check_file_shape(self, file):\n","        file_path = os.path.join(self.data_dir, file)\n","        data = np.load(file_path)['arr_0']\n","        return data.shape == (166, 166, 30)\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        file_path = os.path.join(self.data_dir, self.files[idx])\n","        data = np.load(file_path)['arr_0']\n","        data = np.moveaxis(data, -1, 0)  # Move channel to first dimension\n","\n","        data = torch.from_numpy(data).float()  # Convert to PyTorch tensor\n","\n","        if self.augment: #way to improve is to add to the data\n","            data = self.transform(data) #there we should concatenate both data and data_transformed\n","\n","        if self.mean is not None and self.std is not None:\n","            data = np.clip(data, a_min=None, a_max= 1000)\n","            data = (data - self.mean) / self.std\n","\n","\n","        label = 1.0 if 'data1' in self.data_dir else 0.0\n","\n","        return data, np.float32(label)"]},{"cell_type":"markdown","metadata":{"id":"pvVU4cLZ8WJC"},"source":["#### Run to get the Data"]},{"cell_type":"markdown","metadata":{"id":"IR5Cg0Mm5OX1"},"source":["We calculated the mean and std previously with function compute_mean_std"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:47.983385Z","iopub.status.busy":"2023-12-14T17:42:47.982610Z","iopub.status.idle":"2023-12-14T17:42:47.997909Z","shell.execute_reply":"2023-12-14T17:42:47.997118Z","shell.execute_reply.started":"2023-12-14T17:42:47.983360Z"},"id":"ofvsSor18WJD","trusted":true},"outputs":[],"source":["mean_data = 50.564544677734375\n","std_data = 49.94772720336914"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:48.000387Z","iopub.status.busy":"2023-12-14T17:42:48.000131Z","iopub.status.idle":"2023-12-14T17:43:41.483866Z","shell.execute_reply":"2023-12-14T17:43:41.483042Z","shell.execute_reply.started":"2023-12-14T17:42:48.000365Z"},"id":"55A8A8oBm7hS","trusted":true},"outputs":[],"source":["train_data1 = NPZDataset(folder1_path, mean=mean_data, std=std_data, augment=True)\n","train_data0 = NPZDataset(folder0_path, mean=mean_data, std=std_data, augment=True)\n","train_dataset = torch.utils.data.ConcatDataset([train_data1, train_data0])\n","\n","test_data1 = NPZDataset(folder1_test_path, mean=mean_data, std=std_data, augment=False)\n","test_data0 = NPZDataset(folder0_test_path, mean=mean_data, std=std_data, augment=False)\n","test_dataset = torch.utils.data.ConcatDataset([test_data1, test_data0])\n","\n","valid_data1 = NPZDataset(folder1_valid_path, mean=mean_data, std=std_data, augment=False) \n","valid_data0 = NPZDataset(folder0_valid_path, mean=mean_data, std=std_data, augment=False)\n","validate_dataset = torch.utils.data.ConcatDataset([valid_data1, valid_data0])"]},{"cell_type":"markdown","metadata":{"id":"gNeiq6A18WJF"},"source":["#### Divide train, validate, and test data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.564009Z","iopub.status.busy":"2023-12-14T17:43:41.563783Z","iopub.status.idle":"2023-12-14T17:43:41.569002Z","shell.execute_reply":"2023-12-14T17:43:41.568168Z","shell.execute_reply.started":"2023-12-14T17:43:41.563989Z"},"id":"ISiAU_sos-kt","outputId":"6d859288-8ce0-4d95-ada4-daa1298c7a86","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train:  1200\n","validate:  252\n","test:  260\n"]}],"source":["print(\"train: \",len(train_dataset))\n","print(\"validate: \",len(validate_dataset))\n","print(\"test: \",len(test_dataset))"]},{"cell_type":"markdown","metadata":{"id":"DYtnaQCp8WJG"},"source":["DataLoader is designed to iterate over batches of data rather than individual samples, so when we try to access for example the first mini-batch as `train_loader[0]`, we get an error.\n","\n","However, note how before we could iterate over it\n"]},{"cell_type":"markdown","metadata":{"id":"xjoFf4yO8WJH"},"source":["# 2. Define the Neural Network"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:45.716075Z","iopub.status.busy":"2023-12-14T17:43:45.715719Z","iopub.status.idle":"2023-12-14T17:43:45.735102Z","shell.execute_reply":"2023-12-14T17:43:45.734055Z","shell.execute_reply.started":"2023-12-14T17:43:45.716041Z"},"id":"tY5U1rgA8WJH","trusted":true},"outputs":[],"source":["class RCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(30,64,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(64,64,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64,128,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(128,128,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(128,256,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(256,256,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer7 = nn.Sequential(\n","            nn.Conv2d(256,512,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer8 = nn.Sequential(\n","            nn.Conv2d(512,512,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        # hidden_size is an hyperparameter to be adjusted\n","        # try augmenting num_layers\n","        self.lstm = nn.LSTM(input_size=12800, hidden_size=256, num_layers=1, batch_first=True)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256,128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(128,32),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(32,1),\n","            #nn.Sigmoid()     # don't include it as it is already included in BCELogitLoss (BCELoss is less stable)\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.Linear)):\n","                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self,x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.max_pool(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.max_pool(out)\n","        out = self.layer5(out)\n","        out = self.layer6(out)\n","        out = self.max_pool(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = self.max_pool(out)\n","        out = self.layer8(out)\n","        out = self.layer8(out)\n","        out = self.max_pool(out)\n","        out = out.view(out.size(0),-1)\n","        lstm_out, _ = self.lstm(out)\n","        out = self.classifier(lstm_out)\n","\n","        return out\n","\n","    def graph(self): #for visualization and debugging\n","        return nn.Sequential(self.layer1,self.layer2,self.maxPool,self.layer3,self.layer4,self.maxPool,self.layer5,self.layer6,self.maxPool,self.layer7,self.layer8, self.maxPool,self.layer8,self.layer8,self.maxPool,self.classifier)"]},{"cell_type":"markdown","metadata":{"id":"qyB-i0kb8WJI"},"source":["# 3. Cross validation"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1 Training script"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def train_epoch(model, optimizer, scheduler, criterion, train_loader, device, threshold):\n","    model.train()\n","    correct, train_loss, total_length = 0, 0, 0\n","\n","    for i, (data, target) in enumerate(train_loader):\n","\n","        #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n","        data, target = data.to(device), target.to(device).unsqueeze(1).float()\n","\n","        #FORWARD PASS\n","        output = model(data)\n","        loss = criterion(output, target)\n","\n","        #BACKWARD AND OPTIMIZE\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # PREDICTIONS\n","        pred = (torch.sigmoid(output) >= threshold).float()\n","\n","        # PERFORMANCE CALCULATION\n","        train_loss += loss.item() * len(data)\n","        total_length += len(data)\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    scheduler.step()\n","    train_loss = train_loss / total_length\n","    train_acc = correct / total_length\n","\n","    return train_loss, train_acc, scheduler.get_last_lr()[0]"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2 Validation scripts"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def predict(model, train_loader, criterion, device, threshold):\n","    model.eval()\n","    correct, val_loss, total_length = 0, 0, 0\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for data, target in train_loader:\n","\n","            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n","            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n","\n","            #FORWARD PASS\n","            output = model(data)\n","            loss = criterion(output, target)\n","            # PREDICTIONS\n","            pred = (torch.sigmoid(output) >= threshold).float()\n","\n","            # PERFORMANCE CALCULATION\n","            val_loss += loss.item() * len(data)\n","            total_length += len(data)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            all_preds.extend(pred.view(-1).cpu().numpy())\n","            all_targets.extend(target.view(-1).cpu().numpy())\n","\n","    val_loss = val_loss / total_length\n","    val_acc = correct / total_length\n","\n","    return val_loss, val_acc, np.array(all_preds), np.array(all_targets)\n","\n","\n","def test(model, train_loader, criterion, device, threshold):\n","    model.eval()\n","    correct, val_loss, total_length = 0, 0, 0\n","    all_preds = []\n","    all_targets = []\n","    all_out = []\n","\n","    with torch.no_grad():\n","        for data, target in train_loader:\n","\n","            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n","            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n","\n","            #FORWARD PASS\n","            output = model(data)\n","            loss = criterion(output, target)\n","            # PREDICTIONS\n","            pred = (torch.sigmoid(output) >= threshold).float()\n","\n","            # PERFORMANCE CALCULATION\n","            val_loss += loss.item() * len(data)\n","            total_length += len(data)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            all_preds.extend(pred.view(-1).cpu().numpy())\n","            all_targets.extend(target.view(-1).cpu().numpy())\n","            all_out.extend(output.view(-1).cpu().numpy())\n","\n","    val_loss = val_loss / total_length\n","    val_acc = correct / total_length\n","\n","    return val_loss, val_acc, np.array(all_preds), np.array(all_targets), np.array(all_out)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:45:02.371050Z","iopub.status.busy":"2023-12-14T17:45:02.370356Z","iopub.status.idle":"2023-12-14T17:45:04.689628Z","shell.execute_reply":"2023-12-14T17:45:04.688612Z","shell.execute_reply.started":"2023-12-14T17:45:02.371019Z"},"id":"_tquXPNJm7hr","outputId":"729d7b58-e1c1-4889-8590-745e02c5f785","trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-3\u001b[39m\n\u001b[0;32m      6\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m----> 7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m      9\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(validate_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["import json #to store the final dictionnary\n","#hyperparameters remaining constant for cross-validation, will be updated after it\n","num_epochs = 100\n","batch_size = 2\n","weight_decay = 5e-3\n","threshold = 0.5\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#Define hyperparameter crossvalidation\n","initial_lr = [\n","    1e-4#,\n","    #1e-5,\n","    #1e-6\n","]\n","\n","optimizers = [\n","    {'optimizer': torch.optim.SGD, 'weight_decay': weight_decay, 'momentum': 0.9, 'nesterov': True},\n","    {'optimizer': torch.optim.AdamW, 'weight_decay': weight_decay}\n","]\n","\n","schedulers = [\n","    torch.optim.lr_scheduler.ExponentialLR,\n","    torch.optim.lr_scheduler.CosineAnnealingLR,\n","    torch.optim.lr_scheduler.ReduceLROnPlateau\n","]\n","\n","# Store the results of the cross_validation in a dictionnary\n","a=0\n","results_dict={}\n","\n","#loop\n","for lr in initial_lr: #Unnecessary if only one initial lr\n","    for optimizer_params in optimizers:\n","        for scheduler_type in schedulers:\n","\n","            #1. initialize the model\n","            model = RCNN()\n","            model.to(device)\n","            \n","            #2. Define Optimizer \n","            optimizer_class = optimizer_params['optimizer']\n","            optimizer_kwargs = {key: value for key, value in optimizer_params.items() if key != 'optimizer'}\n","            \n","            optimizer = optimizer_class(model.parameters(), lr=lr, **optimizer_kwargs)\n","\n","            #3. Define Scheduler\n","            if scheduler_type==torch.optim.lr_scheduler.ExponentialLR:\n","                scheduler_params= dict(\n","                    gamma=0.9\n","                )\n","\n","            elif scheduler_type==torch.optim.lr_scheduler.CosineAnnealingLR:\n","                scheduler_params= dict(\n","                    T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size\n","                )\n","\n","            elif scheduler_type==torch.optim.lr_scheduler.ReduceLROnPlateau:\n","                scheduler_params= dict(\n","                    mode='min', \n","                    factor=0.1, \n","                    patience=10, \n","                    threshold=1e-4, \n","                    threshold_mode='rel', \n","                    cooldown=0, \n","                    min_lr=0, \n","                    eps=1e-8, \n","                    verbose=False\n","                )\n","            else:\n","                print(f\"problem with the scheduler :{scheduler_type}\")\n","                exit()\n","            \n","            scheduler = scheduler_type(optimizer, **scheduler_params)\n","\n","            # Logs results\n","            loss_history_train=[]\n","            lr_history_train=[]\n","            acc_history_train=[]\n","\n","            acc_history_test=[]\n","            loss_history_test=[]\n","\n","            f1_val_history = []\n","\n","            print(f\"Start of the training model {a} over 6\")\n","\n","            # Loop to update and record learning rate\n","            for epoch in range(1,num_epochs+1):\n","                # Train\n","                train_loss, train_acc, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, device, threshold)\n","                loss_history_train.append(train_loss)\n","                acc_history_train.append(train_acc)\n","                lr_history_train.append(lrs)\n","\n","                # Validate\n","                val_loss, val_acc, val_preds, val_targets = predict(model, valid_loader, criterion, device, threshold)\n","                loss_history_test.append(val_loss)\n","                acc_history_test.append(val_acc)\n","\n","                # Calculate F1 score on validation\n","                f1 = f1_score(val_targets, val_preds, average='binary')  # adjust the average parameter as needed\n","                f1_val_history.append(f1)\n","\n","                if epoch == 50:\n","                    print(f\"epoch 50: valid acc ={acc_history_test[-1]}, f1 val = {f1_val_history[-1]}\")\n","\n","            #Calculate results on test set\n","            test_loss, test_accuracy, test_predictions, test_targets, test_out = test(model, test_loader, criterion, device, threshold)\n","            test_f1_score = f1_score(test_targets, test_predictions, average='binary')\n","            \n","            #store results in a dictionnary\n","\n","            #Accuracy on train set (100 values per model)\n","            acc_train_name = f\"acc_train_{a}\"\n","            results_dict[acc_train_name] = acc_history_train\n","\n","            #F1 score on validation set (100 values)  \n","            f1_valid_name = f\"f1_valid_{a}\"\n","            results_dict[f1_valid_name] = f1_val_history\n","\n","            #F1 score on test set (1 value)\n","            f1_test_name = f\"f1_test_{a}\"\n","            results_dict[f1_test_name] = test_f1_score\n","\n","            #update variable\n","            a+=1\n","\n","with open('result_cv.json', 'w') as file:\n","    json.dump(results_dict, file)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Plots With the result dictionnary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#We have one value of f1_test, 100 values of f1_valid and acc_train for each model (18)\n","\n","#Plots f1 score (y) against epochs=np.arange(1,101) (x), 18 lines as 18 models (maybe reduce it by initial learning rate)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4165525,"sourceId":7201277,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
