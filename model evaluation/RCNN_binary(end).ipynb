{"cells":[{"cell_type":"markdown","metadata":{"id":"RvPPmFWrm7hG"},"source":["IMPORTANT: for this code use data0 and data1 without separating testing data into a separate folder"]},{"cell_type":"markdown","metadata":{"id":"g0PfonFkm7hG"},"source":["<div style=\"width: 1350px; height: 30px; background-color: green;\"></div>\n"]},{"cell_type":"markdown","metadata":{"id":"p3xO2wwt8WI_"},"source":["# 0. Load Modules"]},{"cell_type":"markdown","metadata":{"id":"tZ7mpr4b8WI_"},"source":["Convert to py:\n","\n","`jupyter nbconvert --to script RCNN_crossval.ipynb`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:41.466005Z","iopub.status.busy":"2023-12-14T17:42:41.465193Z","iopub.status.idle":"2023-12-14T17:42:47.958984Z","shell.execute_reply":"2023-12-14T17:42:47.958188Z","shell.execute_reply.started":"2023-12-14T17:42:41.465970Z"},"id":"dvMqFAyy8WJA","trusted":true},"outputs":[],"source":["# Core Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Operating System Interaction\n","import os\n","import sys\n","\n","# Machine Learning Frameworks\n","import torch\n","from torchvision import datasets\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Data Transformation and Augmentation (not all of these transformations were finally used)\n","from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, \\\n","    RandomVerticalFlip, ColorJitter, RandomAffine, RandomPerspective, RandomResizedCrop, \\\n","    GaussianBlur, RandomAutocontrast\n","from torchvision.transforms import functional as F\n","\n","# Model Building and Initialization\n","import torch.nn as nn\n","from torch.nn.init import kaiming_normal_\n","\n","# Data Loading and Dataset Handling\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import random_split, Subset\n","from PIL import Image\n","\n","# Cross-Validation and Metrics\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, roc_curve, auc, accuracy_score\n","from scipy.special import expit as sigmoid\n","\n","# Visualization and Display\n","from matplotlib.animation import FuncAnimation\n","from matplotlib.colors import Normalize\n","from IPython.display import HTML\n","\n","# Miscellaneous\n","import random\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:47.961169Z","iopub.status.busy":"2023-12-14T17:42:47.960754Z","iopub.status.idle":"2023-12-14T17:42:47.966293Z","shell.execute_reply":"2023-12-14T17:42:47.965443Z","shell.execute_reply.started":"2023-12-14T17:42:47.961136Z"},"id":"tTmZUgya8WJB","outputId":"573aa256-a204-48f9-b12e-210a0e045fe3","trusted":true},"outputs":[],"source":["# For Google Colab, mount Google Drive, for local environments, get local path (github)\n","\n","# Change with the appropriate path. Log in into Drive and create the folders with the data\n","\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Carlos\n","    #folder0_path = '/content/drive/My Drive/solar_jets/data0'\n","    #folder0_test_path = '/content/drive/My Drive/solar_jets/data0_test'\n","    #folder1_path = '/content/drive/My Drive/solar_jets/data1'\n","    #folder1_test_path = '/content/drive/My Drive/solar_jets/data1_test'\n","\n","    # Julie\n","    folder0_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0'\n","    folder1_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1'\n","    folder0_test_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0_test'\n","    folder1_test_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1_test'\n","    folder0_valid_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1_val'\n","    folder1_valid_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1_val'\n","\n","    #Adrien\n","    #folder0_path = '/content/drive/My Drive/Machine Learning/Projet/data0'\n","    #folder1_path = '/content/drive/My Drive/Machine Learning/Projet/data1'\n","    #folder0_test_path = '/content/drive/My Drive/Machine Learning/Projet/data0_test'\n","    #folder1_test_path = '/content/drive/My Drive/Machine Learning/Projet/data1_test'\n","    #folder0_valid_path = '/content/drive/My Drive/Machine Learning/Projet/data0_val'\n","    #folder1_valid_path = '/content/drive/My Drive/Machine Learning/Projet/data1_val'\n","else:\n","    # For local environments like VS Code\n","    folder0_path = '../data/data separated/data0'\n","    folder1_path = '../data/data separated/data1'\n","    folder0_test_path = '../data/data separated/data0_test'\n","    folder1_test_path = '../data/data separated/data1_test'\n","    folder0_valid_path = '../data/data separated/data0_val'\n","    folder1_valid_path = '../data/data separated/data1_val'"]},{"cell_type":"markdown","metadata":{"id":"bcGqScbn8WJC"},"source":["## 1.1. Prepare the dataset"]},{"cell_type":"markdown","metadata":{"id":"sSF0xLC78WJC"},"source":["#### Create the class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:47.967756Z","iopub.status.busy":"2023-12-14T17:42:47.967459Z","iopub.status.idle":"2023-12-14T17:42:47.981529Z","shell.execute_reply":"2023-12-14T17:42:47.980663Z","shell.execute_reply.started":"2023-12-14T17:42:47.967732Z"},"id":"xB4lQoqum7hQ","trusted":true},"outputs":[],"source":["class TensorTransforms:\n","    def __init__(self, rotate_angle=30):\n","        self.rotate_angle = rotate_angle\n","\n","    def random_horizontal_flip(self, x):\n","        if random.random() > 0.5:\n","            return torch.flip(x, [2])  # Flip along width\n","        return x\n","\n","    def random_vertical_flip(self, x):\n","        if random.random() > 0.5:\n","            return torch.flip(x, [1])  # Flip along height\n","        return x\n","\n","    def random_rotation(self, x):\n","        # Random rotation in increments of 90 degrees for simplicity\n","        k = random.randint(0, 3)  # 0, 90, 180, or 270 degrees\n","        return torch.rot90(x, k, [1, 2])  # Rotate along height and width\n","\n","    def __call__(self, x):\n","        x = self.random_horizontal_flip(x)\n","        x = self.random_vertical_flip(x)\n","        x = self.random_rotation(x)\n","        return x\n","\n","class NPZDataset(Dataset):\n","    def __init__(self, data_dir, augment=True, mean=None, std=None):\n","        self.data_dir = data_dir\n","        self.augment = augment\n","        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n","        self.transform = TensorTransforms()\n","        self.mean = mean\n","        self.std = std\n","\n","    def _check_file_shape(self, file):\n","        file_path = os.path.join(self.data_dir, file)\n","        data = np.load(file_path)['arr_0']\n","        return data.shape == (166, 166, 30)\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        file_path = os.path.join(self.data_dir, self.files[idx])\n","        data = np.load(file_path)['arr_0']\n","        data = np.moveaxis(data, -1, 0)  # Move channel to first dimension\n","\n","        data = torch.from_numpy(data).float()  # Convert to PyTorch tensor\n","\n","        if self.augment:\n","            data = self.transform(data)\n","\n","        if self.mean is not None and self.std is not None:\n","            data = np.clip(data, a_min=None, a_max= 1000)\n","            data = (data - self.mean) / self.std\n","\n","\n","        label = 1.0 if 'data1' in self.data_dir else 0.0\n","\n","        return data, np.float32(label)"]},{"cell_type":"markdown","metadata":{"id":"pvVU4cLZ8WJC"},"source":["#### Run to get the Data"]},{"cell_type":"markdown","metadata":{"id":"IR5Cg0Mm5OX1"},"source":["We calculated the mean and std previously with function compute_mean_std"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:47.983385Z","iopub.status.busy":"2023-12-14T17:42:47.982610Z","iopub.status.idle":"2023-12-14T17:42:47.997909Z","shell.execute_reply":"2023-12-14T17:42:47.997118Z","shell.execute_reply.started":"2023-12-14T17:42:47.983360Z"},"id":"ofvsSor18WJD","trusted":true},"outputs":[],"source":["mean_data = 50.564544677734375\n","std_data = 49.94772720336914"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:42:48.000387Z","iopub.status.busy":"2023-12-14T17:42:48.000131Z","iopub.status.idle":"2023-12-14T17:43:41.483866Z","shell.execute_reply":"2023-12-14T17:43:41.483042Z","shell.execute_reply.started":"2023-12-14T17:42:48.000365Z"},"id":"55A8A8oBm7hS","trusted":true},"outputs":[],"source":["train_data1 = NPZDataset(folder1_path, mean=mean_data, std=std_data, augment=True)\n","train_data0 = NPZDataset(folder0_path, mean=mean_data, std=std_data, augment=True)\n","train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.485385Z","iopub.status.busy":"2023-12-14T17:43:41.485035Z","iopub.status.idle":"2023-12-14T17:43:41.489786Z","shell.execute_reply":"2023-12-14T17:43:41.488839Z","shell.execute_reply.started":"2023-12-14T17:43:41.485353Z"},"id":"zgiQUsyzEtsE","trusted":true},"outputs":[],"source":["# Just for debugging\n","# train_data1 = ([train_data1[i] for i in range(100)])\n","# train_data0 = ([train_data0[i] for i in range(100)])\n","# train_data = torch.utils.data.ConcatDataset([train_data1, train_data0])"]},{"cell_type":"markdown","metadata":{"id":"hUpUkTw48WJD"},"source":["Below we'll see how train_data can be treated just like lists\n","\n","- One element in the list for each sequence of images\n","- Each element of the list has a tuple of two elements, the first the array (166,30,30) and the second the label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.491225Z","iopub.status.busy":"2023-12-14T17:43:41.490915Z","iopub.status.idle":"2023-12-14T17:43:41.503293Z","shell.execute_reply":"2023-12-14T17:43:41.502467Z","shell.execute_reply.started":"2023-12-14T17:43:41.491194Z"},"id":"h-oXlEEi8WJD","outputId":"51260d67-a601-4f4b-9c48-a70c54f5120d","trusted":true},"outputs":[],"source":["print(\"samples y=1: \",len(train_data1))\n","print(\"samples y=0: \",len(train_data0))\n","print(\"samples total \",len(train_data))"]},{"cell_type":"markdown","metadata":{"id":"ooTweIk78WJF"},"source":["Comput mean and std for train dataset (do not run this)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.504587Z","iopub.status.busy":"2023-12-14T17:43:41.504342Z","iopub.status.idle":"2023-12-14T17:43:41.518313Z","shell.execute_reply":"2023-12-14T17:43:41.517638Z","shell.execute_reply.started":"2023-12-14T17:43:41.504566Z"},"id":"fZwNPOkq8WJF","trusted":true},"outputs":[],"source":["def compute_mean_std(dataset):\n","    mean = 0.0\n","    std = 0.0\n","    for data, _ in dataset:\n","        mean += data.mean()\n","        std += data.std()\n","    mean /= len(dataset)\n","    std /= len(dataset)\n","    return mean.item(), std.item()\n","\n","# mean, std = compute_mean_std(train_data)\n","# mean, std"]},{"cell_type":"markdown","metadata":{"id":"gNeiq6A18WJF"},"source":["#### Divide train, validate, and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.519924Z","iopub.status.busy":"2023-12-14T17:43:41.519436Z","iopub.status.idle":"2023-12-14T17:43:41.562849Z","shell.execute_reply":"2023-12-14T17:43:41.562168Z","shell.execute_reply.started":"2023-12-14T17:43:41.519894Z"},"id":"U_vh2PCW8WJF","trusted":true},"outputs":[],"source":["train_size = int(0.7 * len(train_data))  # 70% of data for training\n","validate_size = int(0.15 * len(train_data))  # 15% for validation\n","test_size = len(train_data) - train_size - validate_size  # remaining 15% for testing\n","\n","train_dataset, validate_dataset, test_dataset = random_split(train_data, [train_size, validate_size, test_size])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.564009Z","iopub.status.busy":"2023-12-14T17:43:41.563783Z","iopub.status.idle":"2023-12-14T17:43:41.569002Z","shell.execute_reply":"2023-12-14T17:43:41.568168Z","shell.execute_reply.started":"2023-12-14T17:43:41.563989Z"},"id":"ISiAU_sos-kt","outputId":"6d859288-8ce0-4d95-ada4-daa1298c7a86","trusted":true},"outputs":[],"source":["print(\"train: \",len(train_dataset))\n","print(\"validate: \",len(validate_dataset))\n","print(\"test: \",len(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:41.570396Z","iopub.status.busy":"2023-12-14T17:43:41.570040Z","iopub.status.idle":"2023-12-14T17:43:45.714419Z","shell.execute_reply":"2023-12-14T17:43:45.713437Z","shell.execute_reply.started":"2023-12-14T17:43:41.570364Z"},"id":"iBlyxujhm7hX","outputId":"db952dc4-1190-4985-ffd0-27b066c975de","trusted":true},"outputs":[],"source":["index = 12\n","your_array = train_data[index][0].numpy()\n","print(train_data[index][1])\n","\n","# Function to update the plot\n","def update(frame):\n","    im.set_array(your_array[frame])\n","    return [im]\n","\n","# Set up the plot\n","fig, ax = plt.subplots()\n","norm = Normalize(vmin=your_array.min(), vmax=your_array.max())  # Adjust as needed\n","im = ax.imshow(your_array[0], cmap='hot', norm=norm)  # Initialize with the first frame\n","\n","# Create the animation\n","ani = FuncAnimation(fig, update, frames=range(30), blit=True)\n","\n","# Display as HTML5 video in the notebook\n","HTML(ani.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"DYtnaQCp8WJG"},"source":["DataLoader is designed to iterate over batches of data rather than individual samples, so when we try to access for example the first mini-batch as `train_loader[0]`, we get an error.\n","\n","However, note how before we could iterate over it\n"]},{"cell_type":"markdown","metadata":{"id":"xjoFf4yO8WJH"},"source":["# 2. Define the Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:45.716075Z","iopub.status.busy":"2023-12-14T17:43:45.715719Z","iopub.status.idle":"2023-12-14T17:43:45.735102Z","shell.execute_reply":"2023-12-14T17:43:45.734055Z","shell.execute_reply.started":"2023-12-14T17:43:45.716041Z"},"id":"tY5U1rgA8WJH","trusted":true},"outputs":[],"source":["class RCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(30,64,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(64,64,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64,128,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(128,128,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(128,256,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(256,256,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer7 = nn.Sequential(\n","            nn.Conv2d(256,512,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.layer8 = nn.Sequential(\n","            nn.Conv2d(512,512,kernel_size=3,padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        # hidden_size is an hyperparameter to be adjusted\n","        # try augmenting num_layers\n","        self.lstm = nn.LSTM(input_size=12800, hidden_size=256, num_layers=1, batch_first=True)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256,128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(128,32),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(32,1),\n","            #nn.Sigmoid()     # don't include it as it is already included in BCELogitLoss (BCELoss is less stable)\n","        )\n","\n","        # for m in self.modules():\n","        #     if not RCNN:\n","        #         kaiming_normal_(m.weight,nonlinearity=\"relu\")#Kaiming to initialize the weights\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.Linear)):\n","                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self,x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.max_pool(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.max_pool(out)\n","        out = self.layer5(out)\n","        out = self.layer6(out)\n","        out = self.max_pool(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = self.max_pool(out)\n","        out = self.layer8(out)\n","        out = self.layer8(out)\n","        out = self.max_pool(out)\n","        out = out.view(out.size(0),-1)\n","        lstm_out, _ = self.lstm(out)\n","        out = self.classifier(lstm_out)\n","\n","        return out\n","\n","    def graph(self): #for visualization and debugging\n","        return nn.Sequential(self.layer1,self.layer2,self.maxPool,self.layer3,self.layer4,self.maxPool,self.layer5,self.layer6,self.maxPool,self.layer7,self.layer8, self.maxPool,self.layer8,self.layer8,self.maxPool,self.classifier)"]},{"cell_type":"markdown","metadata":{"id":"qyB-i0kb8WJI"},"source":["# 3. Training Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:43:45.736852Z","iopub.status.busy":"2023-12-14T17:43:45.736436Z","iopub.status.idle":"2023-12-14T17:43:45.754984Z","shell.execute_reply":"2023-12-14T17:43:45.754278Z","shell.execute_reply.started":"2023-12-14T17:43:45.736816Z"},"id":"eMUYYp7fm7hg","trusted":true},"outputs":[],"source":["def plot_lr(epochs, lrs):\n","    # Creating subplots\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n","\n","    # Linear scale plot\n","    ax1.plot(epochs, lrs, marker='o')\n","    ax1.set_title('Learning Rate per Epoch (Linear Scale)')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Learning Rate')\n","    ax1.set_xticks(epochs)\n","    ax1.grid(True)\n","\n","    # Log scale plot\n","    ax2.plot(epochs, lrs, marker='o')\n","    ax2.set_title('Learning Rate per Epoch (Log Scale)')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Learning Rate')\n","    ax2.set_yscale('log')\n","    ax2.set_xticks(epochs)\n","    ax2.grid(True)\n","\n","    # Display the plots\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T17:45:02.371050Z","iopub.status.busy":"2023-12-14T17:45:02.370356Z","iopub.status.idle":"2023-12-14T17:45:04.689628Z","shell.execute_reply":"2023-12-14T17:45:04.688612Z","shell.execute_reply.started":"2023-12-14T17:45:02.371019Z"},"id":"_tquXPNJm7hr","outputId":"729d7b58-e1c1-4889-8590-745e02c5f785","trusted":true},"outputs":[],"source":["num_epochs = 100\n","batch_size = 2\n","\n","initial_lr = 1e-4\n","weight_decay = 5e-3\n","\n","threshold = 0.5\n","\n","model = RCNN()\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Function to create optimizer and scheduler\n","def reset_optimizer_scheduler():\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n","    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-10, last_epoch=-1)\n","    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=3, factor=0.75)\n","\n","    return optimizer, scheduler\n","\n","# Initialize optimizer and scheduler\n","optimizer, scheduler = reset_optimizer_scheduler()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# Lists to store epoch numbers and corresponding learning rates\n","epochs = []\n","lrs = []\n","\n","# Loop to update and record learning rate\n","for epoch in range(num_epochs):\n","    optimizer.step()\n","    scheduler.step()\n","    current_lr = optimizer.param_groups[0]['lr']\n","    epochs.append(epoch + 1)\n","    lrs.append(current_lr)\n","\n","plot_lr(epochs, lrs)"]},{"cell_type":"markdown","metadata":{"id":"IUqXkC0g8WJJ"},"source":["# 4. Train Epoch Script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urR48DBX8WJK","trusted":true},"outputs":[],"source":["def train_epoch(model, optimizer, scheduler, criterion, train_loader, device, threshold):\n","    model.train()\n","    correct, train_loss, total_length = 0, 0, 0\n","\n","    #for i, (data, target) in tqdm(enumerate(train_loader),desc=\"Epoch no.\"+str(epoch)):\n","    for i, (data, target) in enumerate(train_loader):\n","\n","        #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n","        data, target = data.to(device), target.to(device).unsqueeze(1).float()\n","\n","        #FORWARD PASS\n","        output = model(data)\n","        loss = criterion(output, target)\n","\n","        #BACKWARD AND OPTIMIZE\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # PREDICTIONS\n","        threshold = 0.5\n","        pred = (torch.sigmoid(output) >= threshold).float()\n","\n","        # PERFORMANCE CALCULATION\n","        train_loss += loss.item() * len(data)\n","        total_length += len(data)\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    scheduler.step()\n","    train_loss = train_loss / total_length\n","    train_acc = correct / total_length\n","\n","    return train_loss, train_acc, scheduler.get_last_lr()[0]"]},{"cell_type":"markdown","metadata":{"id":"7uv1vX6J8WJM"},"source":["# 5. Test Script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38S9p-Y88WJN","trusted":true},"outputs":[],"source":["def predict(model, train_loader, criterion, device, threshold):\n","    model.eval()\n","    correct, val_loss, total_length = 0, 0, 0\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for data, target in train_loader:\n","\n","            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n","            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n","\n","            #FORWARD PASS\n","            output = model(data)\n","            loss = criterion(output, target)\n","            # PREDICTIONS\n","            threshold = 0.5\n","            pred = (torch.sigmoid(output) >= threshold).float()\n","\n","            # PERFORMANCE CALCULATION\n","            val_loss += loss.item() * len(data)\n","            total_length += len(data)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            all_preds.extend(pred.view(-1).cpu().numpy())\n","            all_targets.extend(target.view(-1).cpu().numpy())\n","\n","    val_loss = val_loss / total_length\n","    val_acc = correct / total_length\n","\n","    return val_loss, val_acc, np.array(all_preds), np.array(all_targets)\n","\n","\n","def test(model, train_loader, criterion, device, threshold):\n","    model.eval()\n","    correct, val_loss, total_length = 0, 0, 0\n","    all_preds = []\n","    all_targets = []\n","    all_out = []\n","\n","    with torch.no_grad():\n","        for data, target in train_loader:\n","\n","            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n","            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n","\n","            #FORWARD PASS\n","            output = model(data)\n","            loss = criterion(output, target)\n","            # PREDICTIONS\n","            threshold = 0.5\n","            pred = (torch.sigmoid(output) >= threshold).float()\n","\n","            # PERFORMANCE CALCULATION\n","            val_loss += loss.item() * len(data)\n","            total_length += len(data)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            all_preds.extend(pred.view(-1).cpu().numpy())\n","            all_targets.extend(target.view(-1).cpu().numpy())\n","            all_out.extend(output.view(-1).cpu().numpy())\n","\n","    val_loss = val_loss / total_length\n","    val_acc = correct / total_length\n","\n","    return val_loss, val_acc, np.array(all_preds), np.array(all_targets), np.array(all_out)"]},{"cell_type":"markdown","metadata":{"id":"0yKfGhWj8WJP"},"source":["# 6. Train the Network ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5rrUD8WyjWe","outputId":"5e4fc7e1-6813-486a-e8ba-977247c4f6e2","trusted":true},"outputs":[],"source":["# Model\n","model = RCNN() #creates an instance of the RCNN\n","\n","# Loading in consequence\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","# Hyperparameters\n","\n","optimizer, scheduler = reset_optimizer_scheduler()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","\n","# Logs results\n","loss_history_train=[]\n","lr_history_train=[]\n","acc_history_train=[]\n","\n","acc_history_test=[]\n","loss_history_test=[]\n","\n","f1_val_history = []\n","\n","\n","# Early stopping parameters\n","patience = 101\n","best_loss = float('inf')\n","epochs_no_improve = 0\n","\n","# Save best f1 model\n","best_model_state = None\n","best_f1_score = 0\n","\n","\n","# RUN MODEL\n","\n","for epoch in range(1,num_epochs+1):\n","\n","  # Train\n","  train_loss, train_acc, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, device, threshold)\n","  loss_history_train.append(train_loss)\n","  acc_history_train.append(train_acc)\n","  lr_history_train.append(lrs)\n","\n","  # Validate\n","  val_loss, val_acc, val_preds, val_targets = predict(model, valid_loader, criterion, device, threshold)\n","  loss_history_test.append(val_loss)\n","  acc_history_test.append(val_acc)\n","\n","  # Calculate F1 score\n","  f1 = f1_score(val_targets, val_preds, average='binary')  # adjust the average parameter as needed\n","  f1_val_history.append(f1)\n","\n","  print(f\"Epoch {epoch}/{num_epochs} - LR: {lrs:.3e}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val F1 Score: {f1:.4f}\")\n","\n","  # Early stopping\n","  if val_loss < best_loss:\n","    best_loss = val_loss\n","    epochs_no_improve = 0\n","  else:\n","    epochs_no_improve += 1\n","\n","  if epochs_no_improve == patience:\n","    print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n","    break\n","\n","  # Save Model with best F1\n","  if f1 == max(f1_val_history):\n","      best_f1_score = f1\n","      best_model_state = model.state_dict()  # Save the model state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZZ2_one94mo","outputId":"94c3ea5b-c001-4d76-fa59-530d9a79076f","trusted":true},"outputs":[],"source":["# Evaluate on TEST data\n","#test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=train_subsampler)\n","\n","model.load_state_dict(best_model_state)\n","model.to(device)\n","\n","test_loss, test_accuracy, test_predictions, test_targets, test_out = test(model, test_loader, criterion, device, threshold)\n","test_f1_score = f1_score(test_targets, test_predictions, average='binary')  # adjust as needed\n","\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ep-wtC_FVAs","outputId":"beaa0a27-6fa1-480e-c2eb-abe17f995e74","trusted":true},"outputs":[],"source":["accuracy_history=acc_history_test.copy()\n","loss_history=loss_history_test\n","train_loss = loss_history_train.copy()\n","\n","# Use numpy's convolve function to calculate the moving average\n","moving_avg_train = np.convolve(train_loss, np.ones(100)/100, mode='valid')\n","#print(len(train_loss))\n","#print(len(moving_avg_train))\n","\n","n_train = len(accuracy_history)\n","\n","t_test = np.arange(1, num_epochs + 1)\n","t_train = np.linspace(1,num_epochs,len(loss_history_train))\n","\n","fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n","\n","# Plotting training loss history\n","axs[0].semilogy(t_train, train_loss, color='orange', label=\"training loss\")\n","#axs[0].semilogy((t_train[:-99]), moving_avg_train, color='blue', label=\"100-values moving avg\")\n","axs[0].legend()\n","axs[0].set_title('Train loss History')\n","axs[0].set_xlabel('Epoch')\n","axs[0].set_ylabel('Loss')\n","\n","axs[1].plot(t_test, accuracy_history, label=\"Validation\", color='orange')\n","axs[1].set_title('Validation Accuracy History')\n","axs[1].set_xlabel('Epoch')\n","axs[1].set_ylabel('Accuracy')\n","axs[1].legend()\n","\n","axs[2].plot(t_test, lr_history_train)\n","axs[2].set_title('Learning Rate History')\n","axs[2].set_xlabel('Epoch')\n","axs[2].set_ylabel('Learning Rate')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vH1pJNUbm7hx","outputId":"f2e76325-f5b1-44cd-afc8-72f28a8f5749","trusted":true},"outputs":[],"source":["# Compute ROC curve and ROC area for each class\n","fpr, tpr, _ = roc_curve(test_targets, test_out)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plotting the ROC curve\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","\n","# Area Under the Curve (AUC)\n","print(\"Area Under the Curve (AUC): \", roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrKJ4Wj7PeKJ","outputId":"4732f700-5f6e-45f7-ac2f-088257b7cfe8","trusted":true},"outputs":[],"source":["# F1 vs threshold\n","thresholds = np.linspace(0.03, 0.97, 100)  # 100 thresholds between 0 and 1\n","f1_scores = []\n","accuracy_scores = []\n","\n","probabilities = sigmoid(test_out)\n","\n","for thresh in thresholds:\n","    # Convert probabilities to binary predictions based on the threshold\n","    predictions = (probabilities > thresh).astype(int)\n","\n","    # Compute F1 score and Accuracy for this threshold\n","    f1_scores.append(f1_score(test_targets, predictions))\n","    accuracy_scores.append(accuracy_score(test_targets, predictions))\n","\n","\n","\n","plt.figure(figsize=(12, 5))\n","\n","# F1 Score subplot\n","plt.subplot(1, 2, 1)\n","plt.plot(thresholds, f1_scores, marker='')\n","plt.title('F1 Score vs. Thresholds')\n","plt.xlabel('Threshold')\n","plt.ylabel('F1 Score')\n","plt.grid()\n","\n","# Accuracy Score subplot\n","plt.subplot(1, 2, 2)\n","plt.plot(thresholds, accuracy_scores, marker='', color='green')\n","plt.title('Accuracy vs. Thresholds')\n","plt.xlabel('Threshold')\n","plt.ylabel('Accuracy')\n","plt.grid()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# As data is balanced, f1 and accuracy are similar"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4165525,"sourceId":7201277,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
