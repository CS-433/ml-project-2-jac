{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnQt-_vumnL-"
      },
      "source": [
        "# CNN model (from \"Fast Solar Image Classification Using Deep Learning and its Importance for Automation in Solar Physics\" - Convolutional neural network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCDgIqkym_T_"
      },
      "source": [
        "## 0. Load modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsMvUQwfAzrX"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Operating System Interaction\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Machine Learning Frameworks\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Data Transformation and Augmentation (not all of these transformations were finally used)\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, \\\n",
        "    RandomVerticalFlip, ColorJitter, RandomAffine, RandomPerspective, RandomResizedCrop, \\\n",
        "    GaussianBlur, RandomAutocontrast\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# Model Building and Initialization\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_\n",
        "\n",
        "# Data Loading and Dataset Handling\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split, Subset\n",
        "from PIL import Image\n",
        "\n",
        "# Cross-Validation and Metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, roc_curve, auc, accuracy_score\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "# Visualization and Display\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib.colors import Normalize\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Miscellaneous\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnVx_RzCmoLy"
      },
      "source": [
        "## 1. Import models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSzwdjC9nLS5"
      },
      "source": [
        "### 1.1 Declare data path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkofDkmFm4ru",
        "outputId": "c3a209f5-6c22-4bfa-d013-c427336cbf85"
      },
      "outputs": [],
      "source": [
        "# For Google Colab, mount Google Drive, for local environments, get local path (github)\n",
        "\n",
        "# Change with the appropriate path. Log in into Drive and create the folders with the data\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    folder0_path = '... your google path to .../data0'\n",
        "    folder1_path = '... your google path to .../data1'\n",
        "    folder0_test_path = '... your google path to .../data0_test'\n",
        "    folder1_test_path = '... your google path to .../data1_test'\n",
        "    folder0_valid_path = '... your google path to .../data0_val'\n",
        "    folder1_valid_path = '... your google path to .../data1_val'\n",
        "\n",
        "\n",
        "    # Carlos\n",
        "    #folder0_path = '/content/drive/My Drive/solar_jets/data0'\n",
        "    #folder0_test_path = '/content/drive/My Drive/solar_jets/data0_test'\n",
        "    #folder1_path = '/content/drive/My Drive/solar_jets/data1'\n",
        "    #folder1_test_path = '/content/drive/My Drive/solar_jets/data1_test'\n",
        "\n",
        "    # Julie\n",
        "    folder0_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0'\n",
        "    folder1_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1'\n",
        "    folder0_test_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0_test'\n",
        "    folder1_test_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1_test'\n",
        "    folder0_valid_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data0_val'\n",
        "    folder1_valid_path = '/content/drive/My Drive/Colab Notebooks/CS433-project2/data1_val'\n",
        "\n",
        "    #Adrien\n",
        "    #folder0_path = '/content/drive/My Drive/Machine Learning/Projet/data0'\n",
        "    #folder1_path = '/content/drive/My Drive/Machine Learning/Projet/data1'\n",
        "    #folder0_test_path = '/content/drive/My Drive/Machine Learning/Projet/data0_test'\n",
        "    #folder1_test_path = '/content/drive/My Drive/Machine Learning/Projet/data1_test'\n",
        "    #folder0_valid_path = '/content/drive/My Drive/Machine Learning/Projet/data0_val'\n",
        "    #folder1_valid_path = '/content/drive/My Drive/Machine Learning/Projet/data1_val'\n",
        "else:\n",
        "    # For local environments like VS Code\n",
        "    folder0_path = '../data/data separated/data0'\n",
        "    folder1_path = '../data/data separated/data1'\n",
        "    folder0_test_path = '../data/data separated/data0_test'\n",
        "    folder1_test_path = '../data/data separated/data1_test'\n",
        "    folder0_valid_path = '../data/data separated/data0_val'\n",
        "    folder1_valid_path = '../data/data separated/data1_val'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBdJqW0EnPE1"
      },
      "source": [
        "### 1.2 Declare the class and the transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j86rBUk7nHk4"
      },
      "outputs": [],
      "source": [
        "class TensorTransforms:\n",
        "    def __init__(self, rotate_angle=30):\n",
        "        self.rotate_angle = rotate_angle\n",
        "\n",
        "    def random_horizontal_flip(self, x):\n",
        "        if random.random() > 0.5:\n",
        "            return torch.flip(x, [2])  # Flip along width\n",
        "        return x\n",
        "\n",
        "    def random_vertical_flip(self, x):\n",
        "        if random.random() > 0.5:\n",
        "            return torch.flip(x, [1])  # Flip along height\n",
        "        return x\n",
        "\n",
        "    def random_rotation(self, x):\n",
        "        # Random rotation in increments of 90 degrees for simplicity\n",
        "        k = random.randint(0, 3)  # 0, 90, 180, or 270 degrees\n",
        "        return torch.rot90(x, k, [1, 2])  # Rotate along height and width\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.random_horizontal_flip(x)\n",
        "        x = self.random_vertical_flip(x)\n",
        "        x = self.random_rotation(x)\n",
        "        return x\n",
        "\n",
        "class NPZDataset(Dataset):\n",
        "    def __init__(self, data_dir, augment=True, mean=None, std=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.augment = augment\n",
        "        self.files = [f for f in os.listdir(data_dir) if self._check_file_shape(f)]\n",
        "        self.transform = TensorTransforms()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def _check_file_shape(self, file):\n",
        "        file_path = os.path.join(self.data_dir, file)\n",
        "        data = np.load(file_path)['arr_0']\n",
        "        return data.shape == (166, 166, 30)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
        "        data = np.load(file_path)['arr_0']\n",
        "        data = np.moveaxis(data, -1, 0)  # Move channel to first dimension\n",
        "\n",
        "        data = torch.from_numpy(data).float()  # Convert to PyTorch tensor\n",
        "\n",
        "        if self.augment: #way to improve is to add to the data\n",
        "            data = self.transform(data) #there we should concatenate both data and data_transformed\n",
        "\n",
        "        if self.mean is not None and self.std is not None:\n",
        "            data = np.clip(data, a_min=None, a_max= 1000)\n",
        "            data = (data - self.mean) / self.std\n",
        "\n",
        "\n",
        "        label = 1.0 if 'data1' in self.data_dir else 0.0\n",
        "\n",
        "        return data, np.float32(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpgEJukSnT8B"
      },
      "outputs": [],
      "source": [
        "mean_data = 50.564544677734375\n",
        "std_data = 49.94772720336914"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNjMEErinbHL"
      },
      "source": [
        "### 1.3 Get the data, declare the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2f_LEbZ8pag"
      },
      "outputs": [],
      "source": [
        "train_data1 = NPZDataset(folder1_path, mean=mean_data, std=std_data, augment=True)\n",
        "train_data0 = NPZDataset(folder0_path, mean=mean_data, std=std_data, augment=True)\n",
        "train_dataset = torch.utils.data.ConcatDataset([train_data1, train_data0])\n",
        "\n",
        "test_data1 = NPZDataset(folder1_test_path, mean=mean_data, std=std_data, augment=False)\n",
        "test_data0 = NPZDataset(folder0_test_path, mean=mean_data, std=std_data, augment=False)\n",
        "test_dataset = torch.utils.data.ConcatDataset([test_data1, test_data0])\n",
        "\n",
        "valid_data1 = NPZDataset(folder1_valid_path, mean=mean_data, std=std_data, augment=False)\n",
        "valid_data0 = NPZDataset(folder0_valid_path, mean=mean_data, std=std_data, augment=False)\n",
        "validate_dataset = torch.utils.data.ConcatDataset([valid_data1, valid_data0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a75LHr6B8pag"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "valid_loader = DataLoader(validate_dataset, batch_size=2, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On00kDTMn57D"
      },
      "source": [
        "## 2. Define the RCNN, the predict and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEdvdNE_8pai"
      },
      "outputs": [],
      "source": [
        "class RCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(30,64,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # hidden_size is an hyperparameter to be adjusted\n",
        "        # try augmenting num_layers\n",
        "        self.lstm = nn.LSTM(input_size=12800, hidden_size=256, num_layers=1, batch_first=True)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256,128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(128,32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(32,1),\n",
        "            #nn.Sigmoid()     # don't include it as it is already included in BCELogitLoss (BCELoss is less stable)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.max_pool(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.max_pool(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.max_pool(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.max_pool(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.max_pool(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        lstm_out, _ = self.lstm(out)\n",
        "        out = self.classifier(lstm_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def graph(self): #for visualization and debugging\n",
        "        return nn.Sequential(self.layer1,self.layer2,self.maxPool,self.layer3,self.layer4,self.maxPool,self.layer5,self.layer6,self.maxPool,self.layer7,self.layer8, self.maxPool,self.layer8,self.layer8,self.maxPool,self.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2OY9BwjB_-5"
      },
      "outputs": [],
      "source": [
        "def predict(model, train_loader, criterion, device, threshold):\n",
        "    model.eval()\n",
        "    correct, val_loss, total_length = 0, 0, 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in train_loader:\n",
        "\n",
        "            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n",
        "            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n",
        "\n",
        "            #FORWARD PASS\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            # PREDICTIONS\n",
        "            pred = (torch.sigmoid(output) >= threshold).float()\n",
        "\n",
        "            # PERFORMANCE CALCULATION\n",
        "            val_loss += loss.item() * len(data)\n",
        "            total_length += len(data)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / total_length\n",
        "    val_acc = correct / total_length\n",
        "\n",
        "    return val_loss, val_acc, np.array(all_preds), np.array(all_targets)\n",
        "\n",
        "\n",
        "def test(model, train_loader, criterion, device, threshold):\n",
        "    model.eval()\n",
        "    correct, val_loss, total_length = 0, 0, 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_out = []\n",
        "    f1 = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in train_loader:\n",
        "\n",
        "            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n",
        "            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n",
        "\n",
        "            #FORWARD PASS\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            # PREDICTIONS\n",
        "            pred = (torch.sigmoid(output) >= threshold).float()\n",
        "\n",
        "            # PERFORMANCE CALCULATION\n",
        "            val_loss += loss.item() * len(data)\n",
        "            total_length += len(data)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "            all_out.extend(output.view(-1).cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / total_length\n",
        "    val_acc = correct / total_length\n",
        "\n",
        "    return val_loss, val_acc, np.array(all_preds), np.array(all_targets), np.array(all_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compare the results from the grid search for the hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Import the dictionnary containing the parameter specification, the accuracy and the F1 score for all 18 combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json_file_path = 'result_cv_final.json'\n",
        "\n",
        "# Open the JSON file and load the data into a Python dictionary\n",
        "with open(json_file_path, 'r') as file:\n",
        "    result_dict = json.load(file)\n",
        "\n",
        "# Now, 'data_dictionary' contains the content of your JSON file as a Python dictionary\n",
        "print(result_dict)\n",
        "a = int(len(result_dict) / 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_param(result_dict, i):\n",
        "    \"\"\" \n",
        "    Gets the parameters name as strings to use as label for the plots\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    hyperparams_name = f\"hyperparams_{i}\"\n",
        "    hyperparams = result_dict[hyperparams_name]\n",
        "\n",
        "    lr = hyperparams[0]\n",
        "    scheduler_type = hyperparams[1]\n",
        "\n",
        "    if scheduler_type == \"<class 'torch.optim.lr_scheduler.ExponentialLR'>\":\n",
        "      scheduler_type = 'Exponential LR'\n",
        "\n",
        "    elif scheduler_type == \"<class 'torch.optim.lr_scheduler.CosineAnnealingLR'>\":\n",
        "      scheduler_type = 'Cosine annealing LR'\n",
        "\n",
        "    elif scheduler_type == \"<class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\":\n",
        "      scheduler_type = 'Reduce LR on plateau'\n",
        "\n",
        "    optimizer = hyperparams[2]\n",
        "\n",
        "    if optimizer == \"<class 'torch.optim.sgd.SGD'>\":\n",
        "      optimizer = 'SGD'\n",
        "\n",
        "    elif optimizer ==  \"<class 'torch.optim.adamw.AdamW'>\":\n",
        "      optimizer = 'Adam'\n",
        "\n",
        "    return lr, scheduler_type, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Plot the results (1 plot per learning rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "t_test = np.arange(1, num_epochs + 1)\n",
        "clr_lst = ['chocolate', 'brown', 'firebrick', 'orangered', 'darkorange', 'gold',\n",
        "           'yellowgreen', 'darkolivegreen', \"lime\", \"turquoise\", \"aquamarine\", \"teal\",\n",
        "            'deepskyblue', 'royalblue', 'mediumpurple', 'mediumorchid', 'fuchsia', 'deeppink']\n",
        "\n",
        "lr_set = set()\n",
        "\n",
        "for i in range(a):\n",
        "    hyperparams_name = f\"hyperparams_{i}\"\n",
        "    hyperparams = result_dict[hyperparams_name]\n",
        "\n",
        "    lr_set.add(str(hyperparams[0]))\n",
        "\n",
        "if len(lr_set) == 1:\n",
        "\n",
        "  fig, axs = plt.subplots()\n",
        "\n",
        "  for i in range(a):\n",
        "\n",
        "    f1_valid_name = f\"f1_valid_{i}\"\n",
        "    f1_vals = result_dict[f1_valid_name]\n",
        "\n",
        "    lr, scheduler_type, optimizer = get_param(result_dict, i)\n",
        "\n",
        "    axs.plot(t_test, f1_vals, color=clr_lst[i], label= lr + \", \" + scheduler_type + \", \" + optimizer)\n",
        "\n",
        "  axs.legend()\n",
        "  axs.set_title('F1 score on the validation set')\n",
        "  axs.set_xlabel('Epoch')\n",
        "  axs.set_ylabel('F1 score')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "elif len(lr_set) == 2:\n",
        "\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
        "  fig.suptitle('Validation F1 score')\n",
        "\n",
        "  for i in range(int(a/2)):\n",
        "\n",
        "    f1_valid_name = f\"f1_valid_{i}\"\n",
        "    f1_vals = result_dict[f1_valid_name]\n",
        "\n",
        "    lr, scheduler_type, optimizer = get_param(result_dict, i)\n",
        "\n",
        "    axs[0].plot(t_test, f1_vals, color=clr_lst[i], label= scheduler_type + \", \" + optimizer)\n",
        "\n",
        "  axs[0].grid(True)\n",
        "  axs[0].legend()\n",
        "  axs[0].set_title(f'Initial learning rate of {1e-4}')\n",
        "  axs[0].set_xlabel('Epoch')\n",
        "  axs[0].set_ylabel('F1 score')\n",
        "\n",
        "  for i in range(int(a/2), int(a)):\n",
        "\n",
        "    f1_valid_name = f\"f1_valid_{i}\"\n",
        "    f1_vals = result_dict[f1_valid_name]\n",
        "\n",
        "    lr, scheduler_type, optimizer = get_param(result_dict, i)\n",
        "\n",
        "    axs[1].plot(t_test, f1_vals, color=clr_lst[i], label= scheduler_type + \", \" + optimizer)\n",
        "\n",
        "  axs[1].grid(True)\n",
        "  axs[1].legend()\n",
        "  axs[1].set_title(f'Initial learning rate of {1e-5}')\n",
        "  axs[1].set_xlabel('Epoch')\n",
        "  axs[1].set_ylabel('F1 score')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "elif len(lr_set) == 3:\n",
        "\n",
        "  fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "  for i in range(int(a/3)):\n",
        "\n",
        "    f1_valid_name = f\"f1_valid_{i}\"\n",
        "    f1_vals = result_dict[f1_valid_name]\n",
        "\n",
        "    lr, scheduler_type, optimizer = get_param(result_dict, i)\n",
        "\n",
        "    axs[0].plot(t_test, f1_vals, color=clr_lst[i], label= scheduler_type + \", \" + optimizer)\n",
        "\n",
        "  axs[0].grid(True)\n",
        "  axs[0].legend()\n",
        "  axs[0].set_title(\"Initial learning rate of $10^{-4}$\")\n",
        "  axs[0].set_xlabel('Epoch')\n",
        "  axs[0].set_ylabel('F1 score')\n",
        "\n",
        "  for i in range(int(a/3), int(2*a/3)):\n",
        "\n",
        "    f1_valid_name = f\"f1_valid_{i}\"\n",
        "    f1_vals = result_dict[f1_valid_name]\n",
        "\n",
        "    lr, scheduler_type, optimizer = get_param(result_dict, i)\n",
        "\n",
        "    axs[1].plot(t_test, f1_vals, color=clr_lst[i], label= scheduler_type + \", \" + optimizer)\n",
        "\n",
        "  axs[1].grid(True)\n",
        "  axs[1].legend()\n",
        "  axs[1].set_title(\"Initial learning rate of $10^{-5}$\")\n",
        "  axs[1].set_xlabel('Epoch')\n",
        "  axs[1].set_ylabel('F1 score')\n",
        "\n",
        "\n",
        "  for i in range(int(2*a/3), int(3*a/3)):\n",
        "    f1_valid_name = f\"f1_valid_{i}\"\n",
        "    f1_vals = result_dict[f1_valid_name]\n",
        "\n",
        "    lr, scheduler_type, optimizer = get_param(result_dict, i)\n",
        "\n",
        "    axs[2].plot(t_test, f1_vals, color=clr_lst[i], label= scheduler_type + \", \" + optimizer)\n",
        "\n",
        "  axs[2].grid(True)\n",
        "  axs[2].legend()\n",
        "  axs[2].set_title(\"Initial learning rate of $10^{-6}$\")\n",
        "  axs[2].set_xlabel('Epoch')\n",
        "  axs[2].set_ylabel('F1 score')\n",
        "\n",
        "  fig.savefig('figures/crossval.eps', format='eps', bbox_inches='tight')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Import the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt4qLCg48paj"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    #drive.mount('/content/drive')\n",
        "    model_path = '... your google path to ... /Trained_RCNN.pth'\n",
        "    model2_path = '... your google path to .../Trained_RCNN_2.pth'\n",
        "else:\n",
        "  model_path = './Trained_RCNN.pth'\n",
        "  model2_path = './Trained_RCNN_2.pth'\n",
        "\n",
        "model = torch.load(model_path, map_location=device)\n",
        "model2 = torch.load(model2_path, map_location=device)\n",
        "\n",
        "model.eval()\n",
        "model2.eval()\n",
        "\n",
        "model = model.to(device)\n",
        "model2 = model2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9HWErwN8pak"
      },
      "source": [
        "## 5. F1 score vs threshold for the 2 best models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "BtKKpU7f8pak",
        "outputId": "e227b96b-7ca5-4cf2-82b1-8160d2b6020e"
      },
      "outputs": [],
      "source": [
        "thresholds = np.linspace(0, 1, 100)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "print(device)\n",
        "\n",
        "\"\"\" Model 1 \"\"\"\n",
        "\n",
        "model_f1 = np.zeros(len(thresholds))\n",
        "model_acc = np.zeros(len(thresholds))\n",
        "\n",
        "for i,t in enumerate(thresholds):\n",
        "    _, acc, pred, target, _ = test(model, valid_loader, criterion, device, t)\n",
        "    f1 = f1_score(target, pred, average='binary')\n",
        "    print(f\"model 1, f1 = {f1}, acc = {acc} \\n\")\n",
        "    model_f1[i] = f1\n",
        "    model_acc[i] = acc\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Model 2 \"\"\"\n",
        "\n",
        "model2_f1 = np.zeros(len(thresholds))\n",
        "model2_acc = np.zeros(len(thresholds))\n",
        "\n",
        "for i,t in enumerate(thresholds):\n",
        "    _, acc, pred, target, _ = test(model2, valid_loader, criterion, device, t)\n",
        "    f1 = f1_score(target, pred, average='binary')\n",
        "    print(f\"model 2, threshold:  {i}, f1 = {f1}, acc = {acc} \\n\")\n",
        "    model2_f1[i] = f1\n",
        "    model2_acc[i] = acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDzuoVTb8pak",
        "outputId": "4294d2f5-3529-43a7-9cfe-2a60e4e44dbb"
      },
      "outputs": [],
      "source": [
        "best_idx1 = np.argmax(model_f1)\n",
        "best_idx2 = np.argmax(model2_f1)\n",
        "\n",
        "print(f\"The higher F1-score of model 1 is {model_f1[best_idx1]} and corresponds to a threshold value of {thresholds[best_idx1]:.4}. \\n \")\n",
        "print(f\"The higher F1-score of model 2 is {model2_f1[best_idx2]} and corresponds to a threshold value of {thresholds[best_idx2]:.4}. \\n \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Plot the results: F1 score and accuracy vs threshold for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "gdBbzfkX8pak",
        "outputId": "cef14edb-30c8-4ec4-d63a-89b6a3d0b055"
      },
      "outputs": [],
      "source": [
        "\"\"\" F1 score \"\"\"\n",
        "\n",
        "fig, axs = plt.subplots()\n",
        "\n",
        "axs.plot(thresholds, model_f1, color='orange', label='Cosine annealing LR, Adam')\n",
        "axs.plot(thresholds, model2_f1, color='orangered', label='Exponential LR, Adam')\n",
        "\n",
        "axs.set_xlabel('Threshold value')\n",
        "axs.set_ylabel('F1 score')\n",
        "axs.grid(True)\n",
        "axs.legend()\n",
        "\n",
        "fig.savefig('f1_vs_threshold.eps', format='eps')\n",
        "\n",
        "\n",
        "\"\"\" Accuracy \"\"\"\n",
        "\n",
        "fig2, axs2 = plt.subplots()\n",
        "\n",
        "axs2.plot(thresholds, model_acc, color='orange', label='Cosine annealing LR, Adam')\n",
        "axs2.plot(thresholds, model2_acc, color='orangered', label='Exponential LR, Adam')\n",
        "\n",
        "axs2.set_xlabel('Threshold value')\n",
        "axs2.set_ylabel('F1 score')\n",
        "axs2.grid(True)\n",
        "axs2.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWwwCsf48pal"
      },
      "source": [
        "## 6. Define, run and analyze best model (on test set now that this model is the final version!!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0kpbZqj-8sc"
      },
      "outputs": [],
      "source": [
        "# Define final model and threshold for further analysis\n",
        "best_model = model\n",
        "best_thres = 0.1\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Run the final model on test data and get the final F1 score\n",
        "loss, accuracy, pred, target, _ = test(best_model, test_loader, criterion, device, best_thres)\n",
        "test_f1 = f1_score(target, pred, average='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UnX_WFdDBKV",
        "outputId": "9257fd18-812f-4428-fd73-285bf86bbd88"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy of the model on the test set: {accuracy:.4}\")\n",
        "print(f\"Loss of the model on the test set: {loss:.4}\")\n",
        "print(f\"F1 score on test set: {test_f1:.4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mfgbYo_NQ1"
      },
      "source": [
        "## 6.1 Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "YSZ2h6uY8pal",
        "outputId": "ad1312a7-64e7-44e9-9098-f63388460796"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(target, pred)\n",
        "\n",
        "# Create a heatmap for visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Class 0\", \"Class 1\"],\n",
        "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9NJGRkv_q22"
      },
      "source": [
        "## 6.2 Misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hsg3BBQJ-od",
        "outputId": "e858178c-15cd-403e-ff40-945c9acfec6d"
      },
      "outputs": [],
      "source": [
        "#!pip install sunpy         # if running in colab, otherwise sunpy should be installed\n",
        "import sunpy\n",
        "\n",
        "from torchvision import transforms\n",
        "import sunpy.visualization.colormaps as cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTQTy8-a8pal",
        "outputId": "1cd47628-8bdf-44be-f5e2-f83598b27162"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model = model.to(device)\n",
        "threshold = 0.1\n",
        "\n",
        "misclassified_images = []\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "count=0\n",
        "itera=0\n",
        "# Iterate through the test set\n",
        "with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            itera+=1\n",
        "            #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n",
        "            data, target = data.to(device), target.to(device).unsqueeze(1).float()\n",
        "\n",
        "            #FORWARD PASS\n",
        "            output = model(data)\n",
        "            # PREDICTIONS\n",
        "            pred = (torch.sigmoid(output) >= threshold).float().view(-1).cpu()\n",
        "            target=target.view(-1).cpu()\n",
        "\n",
        "            # Identify misclassified images\n",
        "            misclassified_mask = pred != target\n",
        "            count+=misclassified_mask.sum()\n",
        "\n",
        "\n",
        "            misclassified_images.extend(data[misclassified_mask])\n",
        "            true_labels.extend(target[misclassified_mask])\n",
        "            predicted_labels.extend(pred[misclassified_mask])\n",
        "\n",
        "print(f\"There is {count} missclassified images\")\n",
        "# Convert lists to PyTorch tensors\n",
        "misclassified_images = torch.stack(misclassified_images)\n",
        "true_labels = torch.stack(true_labels)\n",
        "predicted_labels = torch.stack(predicted_labels)\n",
        "\n",
        "# Assuming you have a function to reverse normalization if applied during data preprocessing\n",
        "# Replace reverse_normalize with your actual function\n",
        "# Example:\n",
        "reverse_normalize = transforms.Compose([transforms.Normalize(mean=0, std=(1/62.7087)),\n",
        "                                        transforms.Normalize(mean=-51.6644, std=1)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2.2 Generate .gif animations of the 20 misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(dpi=400, figsize=(10, 8))\n",
        "\n",
        "# Function to update the animation frame\n",
        "def update(frame):\n",
        "    im.set_array(image[frame, :, :])\n",
        "    return im,\n",
        "\n",
        "# loop over all misclassified images\n",
        "\n",
        "for i in range(20):\n",
        "\n",
        "    # Get the image sequence and the corresponding label and prediction\n",
        "    image = misclassified_images[i].cpu().numpy()\n",
        "    true_label = true_labels[i].item()\n",
        "    predicted_label = predicted_labels[i].item()\n",
        "\n",
        "    # Define the image normalization\n",
        "    vmin, vmax = np.percentile(image, [1, 99.9])\n",
        "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
        "\n",
        "    # Initialize the animation with an empty frame\n",
        "    im = ax.imshow(image[0,: , :], cmap='sdoaia304', norm=norm)\n",
        "    ax.set_title(f'True: {true_label}, Predicted: {predicted_label}')\n",
        "    \n",
        "    # Create the animation\n",
        "    ani = FuncAnimation(fig, update, frames=30, interval=100, blit=True)  # 100 milliseconds per frame\n",
        "    \n",
        "    # Save in .gif format\n",
        "    print(f\"saving figure {i+1}\")\n",
        "    ani.save(f\"figures/animation_{i+1}.gif\", writer='pillow', fps=10, dpi=400)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "loss, accuracy, pred, target, _ = test(best_model, test_loader, criterion, device, best_thres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr, tpr, _ = roc_curve(target, pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='deepskyblue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.savefig('figures/ROC.eps', format='eps')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
